{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Standard Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "import seaborn as sns\n",
    "import torch\n",
    "import torch.nn as nn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Constants\n",
    "Listed below is the persistent seed I am choosing to use for this project along with some code to set it in all its instances. There is also some code for some standard column names that will be used many times in this project."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SEED = 42\n",
    "\n",
    "np.random.seed(SEED)\n",
    "torch.manual_seed(SEED)\n",
    "torch.cuda.manual_seed(SEED)\n",
    "torch.backends.cudnn.deterministic = True\n",
    "\n",
    "catechins = ['Catechin', 'Epicatechin', 'Gallocatechin', 'Epigallocatechin',\n",
    "       'Catechin Gallate', 'Epicatechin Gallate', 'Gallocatechin Gallate',\n",
    "       'Epigallocatechin Gallate']\n",
    "chemicals = ['Catechin', 'Epicatechin', 'Gallocatechin', 'Epigallocatechin',\n",
    "       'Catechin Gallate', 'Epicatechin Gallate', 'Gallocatechin Gallate',\n",
    "       'Epigallocatechin Gallate', 'Caffeine']\n",
    "sensory_evaluations = ['Taste','Appearance','Aroma','Liqour color','Infused leaf']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import data\n",
    "This data is collected from a wide range of academic articles which bulk work consists of tea-catechins or sensory appeal."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Chemical and Instrumental Assessment of Green Tea Sensory Preference - Y.R. Liang, Q. Ye, J. Jin, H. Liang, J.L. Lu, Y.Y. Du & J.J. Dong\n",
    "# https://doi.org/10.1080/10942910701299430\n",
    "# Notes: downloaded CSVs\n",
    "\n",
    "df_10942910701299430 = pd.read_csv('../data/10942910701299430/chemical_composition.csv', header=1)\n",
    "df_10942910701299430 = pd.concat([df_10942910701299430, pd.read_csv('../data/10942910701299430/sensory_evaluation.csv')], axis=1)\n",
    "\n",
    "# Phytochemical Composition and Antioxidant Capacity of 30 Chinese Teas - by Guo-Yi Tang, Cai-Ning Zhao, Xiao-Yu Xu, Ren-You Gan, Shi-Yu Cao, Qing Liu, AoShang, Qian-Qian Mao & Hua-Bin Li\n",
    "# https://doi.org/10.3390/antiox8060180\n",
    "# Notes: Manually scraped data\n",
    "\n",
    "df_antiox8060180 = pd.read_csv('../data/antiox8060180/chemical_composition.csv')\n",
    "\n",
    "# Catechin and caffeine content of tea (Camellia sinensis L.) leaf significantly differ with seasonal variation... - Himangshu Deka, Tupu Barman, Jintu Dutta, Arundhuti Devi, Pradip Tamuly, Ranjit Kumar Paul & Tanmoy Karak \n",
    "# https://doi.org/10.1016/j.jfca.2020.103684\n",
    "# https://krishi.icar.gov.in/jspui/bitstream/123456789/68751/2/S0889157520313892-main.pdf - Free access\n",
    "\n",
    "# df_j.jfca.2020.103684_chemical_composition = pd.read_csv('dava/j.jfca.2020.103684/chemical_composition.csv')\n",
    "\n",
    "# Comparative analysis of tea catechins and theaflavins by high-performance liquid chromatography and capillary electrophoresis - Bee-Lan Lee & Choon-Nam Ong\n",
    "# https://doi.org/10.1016/S0021-9673(00)00215-6\n",
    "# Notes: Manually scraped data\n",
    "\n",
    "df_S0021967300002156 = pd.read_csv('../data/S0021967300002156/chemical_composition.csv')\n",
    "\n",
    "# Survey of Catechins, Gallic Acid, and Methylxanthines in Green, Oolong, Pu-erh, and Black Teas - Jen-Kun Lin, Chih-Li Lin, Yu-Chih Liang, Shoei-Yn Lin-Shiau & I-Ming Juan\n",
    "# https://doi.org/10.1021/jf980223x\n",
    "# Notes: Manually scraped data\n",
    "\n",
    "df_jf980223x = pd.read_csv('../data/jf980223x/chemical_composition.csv')\n",
    "\n",
    "# Analysis of some selected catechins and caffeine in green tea by high performance liquid chromatography\n",
    "# https://doi.org/10.1016/j.foodchem.2012.03.039\n",
    "# Notes: Manually scraped data\n",
    "\n",
    "df_jfoodchem201203039 = pd.read_csv('../data/jfoodchem201203039/chemical_composition.csv', on_bad_lines='skip')\n",
    "\n",
    "# Catechin content of 18 teas and a green tea extract supplement correlates with the antioxidant capacity \n",
    "# https://doi.org/10.1207/S15327914NC4502_13\n",
    "# Notes: Manually scraped data\n",
    "\n",
    "df_15327914NC4502_13 = pd.read_csv('../data/15327914NC4502_13/chemical_composition.csv')\n",
    "\n",
    "# Catechin contents in tea (Camellia sinensis) as affected by cultivar and environment and their relation to chlorophyll contents\n",
    "# https://doi.org/10.1016/j.foodchem.2010.08.029\n",
    "# Notes: Manually scraped data\n",
    "\n",
    "df_jfoodchem201008029 = pd.read_csv('../data/jfoodchem201008029/chemical_composition.csv')\n",
    "\n",
    "# Analysis of Catechin Content of Commercial Green Tea Products\n",
    "# https://doi.org/10.1080/J157v03n03_03\n",
    "# Notes: Manually scraped data\n",
    "\n",
    "df_J157v03n03_03 = pd.read_csv('../data/J157v03n03_03/chemical_composition.csv')\n",
    "\n",
    "# Phenol-Explorer database scraping\n",
    "# phenol-explorer.eu\n",
    "# Notes: Manually scraped data\n",
    "\n",
    "df_phenol_explorer = pd.read_csv('../data/phenol_explorer/chemical_composition.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# List of all imported dataframes\n",
    "all_dataframes = [df_10942910701299430,\n",
    "                  df_antiox8060180,\n",
    "                  df_S0021967300002156,\n",
    "                  df_jf980223x,\n",
    "                  df_jfoodchem201203039,\n",
    "                  df_15327914NC4502_13,\n",
    "                  df_phenol_explorer]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Combine data\n",
    "\n",
    "To combine the data I am going to create a data pipeline that does the following:\n",
    "\n",
    "1. Add all the pandas dataframes to a list.\n",
    "2. Read that list into a function.\n",
    "3. The function reads an individual dataframe from that list then resets the index, rename catechins to be like the catechin dictionary below, drop unused columns, add columns that are part of the standardized column format, and then sorts the columns to be part of the standardized column format.\n",
    "4. The function will then add the dataframe to a list denoting it is processed.\n",
    "5. Once the function has processed all the dataframes in the lists it will loop through the data frames in the finished list and concatenate them.\n",
    "6. The function will return a combined dataframe.\n",
    "\n",
    "## Data Frame Format\n",
    "\n",
    "The data frames will be combined into the following format:\n",
    "\n",
    "- `'Catechin'`\n",
    "- `'Epicatechin'`\n",
    "- `'Gallocatechin'`\n",
    "- `'Epigallocatechin'`\n",
    "- `'Catechin Gallate'`\n",
    "- `'Epicatechin Gallate'`\n",
    "- `'Gallocatechin Gallate'`\n",
    "- `'Epigallocatechin Gallate'`\n",
    "- `'Gallic Acid'`\n",
    "- `'Chlorogenic Acid'`\n",
    "- `'Caffeine'`\n",
    "- `'Taste'`\n",
    "- `'Appearance'`\n",
    "- `'Aroma'`\n",
    "- `'Liqour color'`\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "catechin_dictionary = {\n",
    "    'C': 'Catechin',\n",
    "    'EC': 'Epicatechin',\n",
    "    'GC': 'Gallocatechin',\n",
    "    'EGC': 'Epigallocatechin',\n",
    "    'CG': 'Catechin Gallate',\n",
    "    'ECG': 'Epicatechin Gallate',\n",
    "    'GCG': 'Gallocatechin Gallate',\n",
    "    'EGCG': 'Epigallocatechin Gallate'\n",
    "}\n",
    "\n",
    "standardized_columns = [\n",
    "    'Catechin',\n",
    "    'Epicatechin',\n",
    "    'Gallocatechin',\n",
    "    'Epigallocatechin',\n",
    "    'Catechin Gallate',\n",
    "    'Epicatechin Gallate',\n",
    "    'Gallocatechin Gallate',\n",
    "    'Epigallocatechin Gallate',\n",
    "    'Caffeine',\n",
    "    'Taste',\n",
    "    'Appearance',\n",
    "    'Aroma',\n",
    "    'Liqour color',\n",
    "    'Infused leaf'\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To faciliate scability I am create a sklearn pipeline and process all my data to be concatenated.\n",
    "First I will define the functions that will go into my data preprocessing pipeline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def reset_index(dataframe: pd.DataFrame):\n",
    "    return dataframe.reset_index(drop=1)\n",
    "\n",
    "def rename_columns(dataframe: pd.DataFrame):\n",
    "    return dataframe.rename(columns=catechin_dictionary)\n",
    "\n",
    "def drop_nonstandardized_columns(dataframe: pd.DataFrame):\n",
    "    return dataframe.drop(columns=[column for column in dataframe if column not in standardized_columns])\n",
    "\n",
    "def reformat_columns(dataframe: pd.DataFrame):\n",
    "    return dataframe.reindex(columns=standardized_columns)\n",
    "\n",
    "def remove_standard_deviation_format(dataframe: pd.DataFrame):\n",
    "    for column in dataframe.columns:\n",
    "        if (dataframe[column].dtype.name == 'object') or (dataframe[column].dtype.name == 'string'):\n",
    "            dataframe[column] = dataframe[column].str.replace('Â± \\d*.\\d*', '', regex=True)\n",
    "    return dataframe    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, the data pipeline applies all of those functions to the inputed dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.preprocessing import FunctionTransformer\n",
    "\n",
    "ResetIndexTransformer = FunctionTransformer(reset_index)\n",
    "RenameColumnsTransformer = FunctionTransformer(rename_columns)\n",
    "DropNonstandardizedColumnsTransformer = FunctionTransformer(drop_nonstandardized_columns)\n",
    "ReformatColumnsTransformer = FunctionTransformer(reformat_columns)\n",
    "RemoveStandardDeviationFormat = FunctionTransformer(remove_standard_deviation_format)\n",
    "\n",
    "data_combining_pipeline = make_pipeline(\n",
    "    ResetIndexTransformer,\n",
    "    RenameColumnsTransformer,\n",
    "    DropNonstandardizedColumnsTransformer,\n",
    "    ReformatColumnsTransformer,\n",
    "    RemoveStandardDeviationFormat\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now I am creating a pipeline that transforms each dataframe then concatenates it and returns one dataframe to be used for the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_dataframes(dataframes: list):\n",
    "    processed_dataframes = []\n",
    "    for dataframe in dataframes:\n",
    "        dataframe = data_combining_pipeline.fit_transform(dataframe)\n",
    "        processed_dataframes.append(dataframe)\n",
    "    return processed_dataframes\n",
    "\n",
    "def concatenate_dataframes(dataframes: list):\n",
    "    formated_dataframe = pd.concat(dataframes, ignore_index=True)\n",
    "    return formated_dataframe\n",
    "\n",
    "def change_dtypes(dataframe: pd.DataFrame):\n",
    "    for column in dataframe.columns:\n",
    "        dataframe[column] = pd.to_numeric(dataframe[column], errors='coerce')\n",
    "    return dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ProcessDataframes = FunctionTransformer(process_dataframes)\n",
    "ConcatenateDataframes = FunctionTransformer(concatenate_dataframes)\n",
    "Change_Dtypes = FunctionTransformer(change_dtypes)\n",
    "\n",
    "data_combining_pipeline_processor = make_pipeline(\n",
    "    ProcessDataframes,\n",
    "    ConcatenateDataframes,\n",
    "    Change_Dtypes\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = data_combining_pipeline_processor.transform(all_dataframes)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Remove Extraneous Data\n",
    "It is common for this aggregated dataset to come with columns that contain no data at all, that will be removed here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.dropna(how='all', axis=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Explanation\n",
    "\n",
    "The data we will be examining and training on is purely numerical. This has many advantages in terms of cleaning and training but a few disadvantages that may not be readily appearant. The catechins and caffeine are all listed in milligrams. What is taste, appearance, aroma, and liquor color measured in? Milligrams? No, instead it is much more complicated. Thankfully this is a pretty standardized tea tasting scoring system throughout China so most of our data will reflect that, any data that does not reflect that will be modified into this scoring system. The scoring system is outlined below:\n",
    "\n",
    "\n",
    "    Scoring System: They used a scoring system where the highest possible score for a tea sample was 100. This score was divided into different categories, each representing a different aspect of the tea:\n",
    "        Dry Tea Appearance (10% of the total score): This is about how the tea leaves look when they are dry. The experts looked at the color, size, how tender the leaves were, and how evenly the leaves were twisted.\n",
    "        Aroma (30% of the total score): This is about how the tea smells.\n",
    "        Liquor Color (10% of the total score): This is about the color of the tea when it's steeped in water.\n",
    "        Taste (30% of the total score): This is, of course, about how the tea tastes.\n",
    "        Infused Leaf (20% of the total score): After the tea has been steeped, the experts looked at the leaves again to see their color, size, tenderness, and evenness.\n",
    "\n",
    "    How They Tested:\n",
    "        Dry Tea Appearance: They placed 200 grams of dry tea on a wooden tray (20 cm x 20 cm) and scored it based on the criteria mentioned above.\n",
    "        Preparing the Tea: For testing aroma, liquor color, and taste, they steeped 3 grams of the tea in 150 milliliters of boiling water for 5 minutes.\n",
    "        Tasting and Scoring: The tea was then poured into a special tasting bowl. The experts compared the aroma, color, and taste of the tea to three reference samples they had in their lab to give their scores.\n",
    "        Infused Leaf: Finally, they put the steeped leaves into a porcelain tray with cold water and scored them based on color, size, tenderness, and evenness.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exploratory Data Analysis\n",
    "\n",
    "This pairplot offers a high-level visualization of what is going on internally in the dataset. Next, we will examine further the intricacies of the data before cleaning it further and imputing data using a Generative Adversarial Network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.set_theme()\n",
    "sns.pairplot(df[catechins])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Summary Statistics\n",
    "\n",
    "The summary statistics of the agggregated data is displayed below. It is interesting to note that there are some significant outliers. One such outlier is Epigallocatechin with a max of aroun 100mg."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training Device\n",
    "\n",
    "As I am developing on Linux and training on Windows, this code displays if CUDA functionality is being used or not."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get cpu, gpu or mps device for training.\n",
    "device = (\n",
    "    \"cuda\"\n",
    "    if torch.cuda.is_available()\n",
    "    else \"mps\"\n",
    "    if torch.backends.mps.is_available()\n",
    "    else \"cpu\"\n",
    ")\n",
    "print(f\"Using {device} device\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Split data\n",
    "\n",
    "For this model we will not be using a validation set because of the limited nature of the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "train_df, test_df = train_test_split(df, test_size=0.2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Imputation and Scaling\n",
    "\n",
    "The data imputation for this aggregated dataset will be done in two parts:\n",
    "\n",
    "1. **Chemicals Imputation**:\n",
    "   - The chemicals will be imputed using sklearn's `SimpleImputer`.\n",
    "\n",
    "2. **Min-Max Scaler**:\n",
    "   - A min-max scaler will be applied so the GAN can learn features correctly.\n",
    "\n",
    "3. **Sensory Appeals Generation**:\n",
    "   - The sensory appeals will be generated by a custom `Generative Adversarial Network (GAN)`.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Missing Chemical Imputation with an Iterative Imputer\n",
    "Note this is causing some negative values that need to be fixed, possibly rows with complete missing data is causing this."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.experimental import enable_iterative_imputer\n",
    "from sklearn.impute import IterativeImputer\n",
    "\n",
    "imputer = IterativeImputer(max_iter=100, verbose=2, random_state=42, imputation_order='random', min_value=[0.02, 1.02, 3.46, 0.84, 4.45, 1.61, 0.26, 8.38, 1.97]) # 25th percentile rounded, may change\n",
    "\n",
    "train_df[chemicals] = imputer.fit_transform(train_df[chemicals])\n",
    "test_df[chemicals] = imputer.transform(test_df[chemicals])\n",
    "\n",
    "test_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generation of Additional Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neighbors import KNeighborsRegressor\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature Scaling\n",
    "The data is purely numerical, therefore the data will be only normalized. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Normalization: Min-Max Scaler\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "scaler = MinMaxScaler() # Note: I am still not sure if I want to scale the labels.\n",
    "\n",
    "scaled_train_df = scaler.fit_transform(train_df)\n",
    "scaled_test_df = scaler.transform(test_df)\n",
    "\n",
    "scaled_train_df = pd.DataFrame(scaled_train_df, columns=standardized_columns)\n",
    "scaled_test_df = pd.DataFrame(scaled_test_df, columns=standardized_columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaled_train_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generative Adversarial Network\n",
    "\n",
    "The purpose of the GAN is to impute missing sensory values. A GAN is a combination of two neural networks: a generator and a discriminator. The generator will generate numbers that it thinks are the appropiate sensory evaluations based on the givens catechins and caffeine input. The discriminator will then decide if that is a realistic output or not. In the end, all missing data on sensory evaluations will be naturally imputted. Much of this was interpreted from https://pytorch.org/tutorials/beginner/dcgan_faces_tutorial.html although specifically using GAN not DCGAN."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a training dataset for the GAN (rows that include all sensory scores)\n",
    "GAN_train_df = scaled_train_df[(scaled_train_df[sensory_evaluations] > 0).all(axis=1)].dropna(axis=0) # Subset of data containing complete sensory evaluations, right now the GAN does not perform well because of lack of data\n",
    "GAN_test_df = scaled_test_df[(scaled_test_df[sensory_evaluations] > 0).all(axis=1)].dropna(axis=0) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Generator(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Generator, self).__init__()\n",
    "        self.main = nn.Sequential(\n",
    "            # Input Layer\n",
    "            nn.Linear(9, 128),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "\n",
    "            nn.Linear(128, 256),\n",
    "            nn.BatchNorm1d(256),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "\n",
    "            nn.Linear(256, 128),\n",
    "            nn.BatchNorm1d(128),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "\n",
    "            nn.Linear(128, 5),\n",
    "            nn.Sigmoid() # This is used because output values are in the range [0, 1) (after scaling of course).\n",
    "        )\n",
    "\n",
    "\n",
    "    def forward(self, input):\n",
    "        output = self.main(input)\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Discriminator(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Discriminator, self).__init__()\n",
    "        self.main = nn.Sequential(\n",
    "            # Input layer\n",
    "            nn.Linear(14, 16),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "            nn.Dropout(0.3),  # Dropout to prevent overfitting\n",
    "\n",
    "            # Hidden layer\n",
    "            nn.Linear(16, 32),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "            nn.Dropout(0.3),\n",
    "\n",
    "            # Output layer - single neuron for binary classification (real or fake)\n",
    "            nn.Linear(32, 1),\n",
    "            nn.Sigmoid()  # Sigmoid to output a probability\n",
    "        )\n",
    "\n",
    "    def forward(self, input):\n",
    "        output = self.main(input)\n",
    "        return output.view(-1, 1).squeeze(1) # Outputs 1D tensor of probabilities \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You must create a custom torch dataset for it to go into the torch dataloader."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Custom GAN Dataset\n",
    "from torch.utils.data import Dataset\n",
    "\n",
    "class GANDataset(Dataset):\n",
    "    def __init__(self, dataframe):\n",
    "        self.data_frame = dataframe\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data_frame)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        if torch.is_tensor(idx):\n",
    "            idx = idx.tolist()\n",
    "        \n",
    "        sample = self.data_frame.iloc[idx]\n",
    "        sample = torch.tensor(sample.values, dtype=torch.float32)\n",
    "        return sample\n",
    "\n",
    "GAN_dataset = GANDataset(GAN_train_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# GAN Dataloader \n",
    "from torch.utils.data import DataLoader\n",
    "dataloader = DataLoader(GAN_dataset, batch_size=32, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model Creation\n",
    "generator = Generator().to(device)\n",
    "discriminator = Discriminator().to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Optimally, I would like to make all of this a reusable pytorch class. There is no hyperparameter searching like grid-search cv in pytorch so I had to do it manually."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup Adam optimizers for both G and D\n",
    "import torch.optim as optim\n",
    "optimizerD = optim.Adam(discriminator.parameters(), lr=0.001, betas=(0.5, 0.999))  \n",
    "optimizerG = optim.Adam(generator.parameters(), lr=0.0008, betas=(0.5, 0.999))\n",
    "\n",
    "# Initialize the BCELoss function\n",
    "criterion = nn.BCELoss()\n",
    "\n",
    "generator_losses = []\n",
    "discriminator_losses = []\n",
    "epochs = 200\n",
    "\n",
    "def train_GAN():\n",
    "    for epoch in range(epochs):\n",
    "        for i, data in enumerate(dataloader, 0):\n",
    "            \n",
    "            # Seperate data and convert to current device\n",
    "            chemical_data = data[:, :9] # Features seperated\n",
    "            sensory_data = data[:, 9:] # Labels seperated\n",
    "\n",
    "            chemical_data = chemical_data.to(device)\n",
    "            sensory_data = sensory_data.to(device)\n",
    "\n",
    "            # Find current batch size\n",
    "            current_batch_size = chemical_data.size(0)\n",
    "\n",
    "            # Create labels for real and generated data\n",
    "            real_label = torch.full((current_batch_size,), .9, dtype=torch.float, device=device) # Creates a tensor of labels indiciated data is real w/ label smoothing\n",
    "            fake_label = torch.full((current_batch_size,), 0. , dtype=torch.float, device=device) # Creates a tensor of labels indidicated data is generated\n",
    "\n",
    "            # Update Discriminator - Clear accumulated gradients from previous pass\n",
    "            discriminator.zero_grad()\n",
    "\n",
    "            # Train discriminator with real data w/ some notes for myself\n",
    "            real_data = torch.cat((chemical_data, sensory_data), 1) # Same as pd.concat(axis=1)\n",
    "            output = discriminator(real_data).view(-1) # Same as numpy.reshape(-1,1), used to calculate loss\n",
    "            errD_real = criterion(output, real_label)\n",
    "            errD_real.backward() # Perform backpropogation based on errD_real\n",
    "            D_x = output.mean().item() # Outputs average discriminator prediction\n",
    "\n",
    "            # Train discriminator with fake data\n",
    "            fake_sensory_data = generator(chemical_data) \n",
    "            fake_data = torch.cat((chemical_data, fake_sensory_data), 1) # Create a dataset of real chemical data but generated sensory data\n",
    "            output = discriminator(fake_data.detach()).view(-1) # detach is used to ensure gradients aren't computed for the generator during the discriminators backwards pass\n",
    "            errD_fake = criterion(output, fake_label)\n",
    "            errD_fake.backward()\n",
    "            D_G_z1 = output.mean().item() # Outputs average discriminator prediction for fake data\n",
    "\n",
    "            # Update Discriminator weights\n",
    "            errD = errD_real + errD_fake\n",
    "            optimizerD.step()\n",
    "\n",
    "            # Update Generator\n",
    "            generator.zero_grad()\n",
    "            output = discriminator(fake_data).view(-1)\n",
    "            errG = criterion(output, real_label)\n",
    "            errG.backward()\n",
    "            D_G_z2 = output.mean().item()\n",
    "\n",
    "            # Update Generate weights\n",
    "            optimizerG.step()\n",
    "\n",
    "            # Print training log\n",
    "            if i % 50 == 0: # Prints every 50 batches, might change\n",
    "                print(f'[{epoch}/{epochs}][{i}/{len(dataloader)}] '\n",
    "                    f'Loss_D: {errD.item():.4f} Loss_G: {errG.item():.4f} '\n",
    "                    f'D(x): {D_x:.4f} D(G(z)): {D_G_z1:.4f} / {D_G_z2:.4f}')\n",
    "                \n",
    "            # Save losses\n",
    "            generator_losses.append(errG.item())\n",
    "            discriminator_losses.append(errD.item())\n",
    "\n",
    "train_GAN()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_GAN_loss():\n",
    "    plt.figure(figsize=(12,7))\n",
    "    plt.title('Generator and Discriminator Loss During Training')\n",
    "    plt.plot(generator_losses, label='G')\n",
    "    plt.plot(discriminator_losses, label='D')\n",
    "    plt.legend()\n",
    "    plt.xlabel('Iterations')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.show()\n",
    "\n",
    "plot_GAN_loss()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Define rows that need to be imputed\n",
    "These are the rows currently missing sensory data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data_to_impute = torch.tensor(scaled_train_df[(scaled_train_df[sensory_evaluations].isna()).all(axis=1)][chemicals].values, dtype=torch.float32).to(device)\n",
    "test_data_to_impute = torch.tensor(scaled_test_df[(scaled_test_df[sensory_evaluations].isna()).all(axis=1)][chemicals].values, dtype=torch.float32).to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Impute missing sensory data with the GAN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "generator.eval() # Put generator in eval mode\n",
    "\n",
    "with torch.no_grad():\n",
    "    imputed_sensory_data_train = pd.DataFrame(generator(train_data_to_impute).cpu().numpy(), columns=sensory_evaluations)\n",
    "    imputed_sensory_data_test = pd.DataFrame(generator(test_data_to_impute).cpu().numpy(), columns=sensory_evaluations)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame(test_data_to_impute.cpu(), columns=chemicals)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Concatenate imputed values to host dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "imputed_train_df = pd.concat([pd.DataFrame(train_data_to_impute.cpu()), pd.DataFrame(imputed_sensory_data_train)], axis=1).set_axis(standardized_columns, axis='columns')\n",
    "imputed_test_df = pd.concat([pd.DataFrame(test_data_to_impute.cpu()), pd.DataFrame(imputed_sensory_data_test)], axis=1).set_axis(standardized_columns, axis='columns')\n",
    "\n",
    "train_df = pd.concat([GAN_train_df, imputed_train_df], axis=0)\n",
    "test_df = pd.concat([GAN_test_df, imputed_test_df], axis=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature Engineering\n",
    "\n",
    "A derived feature called overall score will be produced from all sensory scores so the models only have to predict a single response."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Derived Feature: Overall Sensory Score\n",
    "def compute_sensory_score(df):\n",
    "    df['Overall Sensory Score'] = df['Taste'] + df['Appearance'] + df['Aroma'] + df['Liqour color'] + df['Infused leaf']\n",
    "    df = df.drop(columns=['Taste','Appearance','Aroma','Liqour color','Infused leaf'])\n",
    "    return df\n",
    "\n",
    "train_df = compute_sensory_score(train_df)\n",
    "test_df = compute_sensory_score(test_df)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PCA (Principal Component Analysis)\n",
    "\n",
    "Because the high-dimensional nature of the data, PCA will be used to reduce the number of features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import KernelPCA\n",
    "\n",
    "kpca = KernelPCA(n_components=9, random_state=42)\n",
    "\n",
    "train_X = kpca.fit_transform(train_df.drop(columns=['Overall Sensory Score']))\n",
    "test_X = kpca.transform(test_df.drop(columns=['Overall Sensory Score']))\n",
    "\n",
    "train_y = train_df[['Overall Sensory Score']]\n",
    "test_y = test_df[['Overall Sensory Score']]\n",
    "\n",
    "# https://stackoverflow.com/questions/29611842/scikit-learn-kernel-pca-explained-variancepca.explained_variance_\n",
    "explained_variance = np.var(train_X, axis=0)\n",
    "explained_variance_ratio = explained_variance / np.sum(explained_variance)\n",
    "exp_var_cumul = np.cumsum(explained_variance_ratio)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare data for Torch-based models\n",
    "train_X_tensor = torch.tensor(train_X, dtype=torch.float64).float()\n",
    "train_y_tensor = torch.tensor(train_y.values, dtype=torch.float64).float()\n",
    "test_X_tensor = torch.tensor(test_X, dtype=torch.float64).float()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import plotly.express as px\n",
    "\n",
    "px.area(\n",
    "    x=range(1, exp_var_cumul.shape[0] + 1),\n",
    "    y=exp_var_cumul,\n",
    "    labels={\"x\": \"# Components\", \"y\": \"Explained Variance\"}\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The explained variance ratio achieved with just 2 components is sufficiently high, making it suitable for visualization in a 3D space. Therefore, I will proceed by focusing on these 2 components for my model training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract first two components\n",
    "train_X = train_X[:, :2]\n",
    "test_X = test_X[:, :2]\n",
    "\n",
    "# Prepare data for Torch-based models\n",
    "train_X_tensor = torch.tensor(train_X, dtype=torch.float64).float()\n",
    "train_y_tensor = torch.tensor(train_y.values, dtype=torch.float64).float()\n",
    "test_X_tensor = torch.tensor(test_X, dtype=torch.float64).float()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Random Forest\n",
    "\n",
    "Pytorch does not have a built in Random Forest class, instead we will use scikit learn's implementation. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model Creation\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "\n",
    "GridSearchCV_random_forest = RandomForestRegressor()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyperparameter Selection\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "parameters_random_forest = {\n",
    "    'n_estimators': [int(x) for x in np.linspace(start=100, stop=1000, num=10)],\n",
    "    'max_depth': [int(x) for x in np.linspace(start=1, stop=6, num=6)],\n",
    "    'min_samples_split': [int(x) for x in np.linspace(start=1, stop=6, num=6)],\n",
    "    'random_state': [42]\n",
    "}\n",
    "\n",
    "clf_random_forest = GridSearchCV(GridSearchCV_random_forest, parameters_random_forest)\n",
    "clf_random_forest.fit(train_X, train_y.values.ravel())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf_random_forest.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model Training\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "Model_Random_Forest = RandomForestRegressor(max_depth=2, min_samples_split=4, n_estimators=100, random_state=42)\n",
    "Model_Random_Forest.fit(train_X, train_y)\n",
    "y_pred_random_forest = Model_Random_Forest.predict(test_X)\n",
    "random_forest_mse = mean_squared_error(test_y, y_pred_random_forest)\n",
    "random_forest_mse"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Multilayer Perceptron\n",
    "This multilayer perceptron outputs a overall sensory score based on input chemicals."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "notes: I ultimately want someone to put in a tea and see if it is generally likeable. This returns the scaled score so I need to return that scaled score into an interpretable one. Next I need to see how I would integrate this model into an interactie web app. Think about that..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model Creation\n",
    "class MLP(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()  \n",
    "        self.main = nn.Sequential(\n",
    "            # First Layer\n",
    "            nn.Linear(2, 16),\n",
    "            nn.ReLU(),\n",
    "\n",
    "            # Second Layer\n",
    "            nn.Linear(16, 32),\n",
    "            nn.ReLU(),\n",
    "\n",
    "            # Output Layer\n",
    "            nn.Linear(32, 1),\n",
    "            nn.ReLU() # Used because the output is greater than 1\n",
    "        )\n",
    "\n",
    "    def forward(self, input):\n",
    "        output = self.main(input)\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training Loop Creation - very rough creation\n",
    "def train_MLP(model, X, y, learning_rate=0.1, n_epochs=100):\n",
    "    losses = []\n",
    "    criterion = nn.MSELoss()\n",
    "    optimizer = optim.Adam(model.parameters(), lr=learning_rate, betas=(0.5, 0.999))\n",
    "    for epoch in range(n_epochs):\n",
    "        model.train()\n",
    "        y_pred = model(X)\n",
    "        loss = criterion(y_pred, y)\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        if epoch % 10 == 0: \n",
    "            print(f'[{epoch}/{n_epochs}]',\n",
    "            f'Loss: {loss.item():.4f}')\n",
    "                 \n",
    "        losses.append(loss.item())\n",
    "\n",
    "def predict_MLP(model, X):\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        device = next(model.parameters()).device\n",
    "        X = X.to(device)\n",
    "        predictions = model(X)\n",
    "    return predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model Training\n",
    "Model_MLP = MLP()\n",
    "train_MLP(Model_MLP, train_X_tensor, train_y_tensor) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model Predictions\n",
    "y_pred_mlp = predict_MLP(Model_MLP, test_X_tensor)\n",
    "y_pred_mlp = y_pred_mlp.detach()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model Evaluation\n",
    "mse_mlp = mean_squared_error(test_y, y_pred_mlp)\n",
    "mse_mlp"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Recurrent Neural Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model Creation - https://blog.floydhub.com/a-beginners-guide-on-recurrent-neural-networks-with-pytorch/\n",
    "class RNN(nn.Module):\n",
    "    def __init__(self, input_size=2, output_size=1, hidden_dim=12, n_layers=1):\n",
    "        super(RNN, self).__init__()\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.n_layers = n_layers\n",
    "\n",
    "        # Layers\n",
    "        self.rnn = nn.RNN(input_size, hidden_dim, n_layers, batch_first=True)\n",
    "        self.fully_connected = nn.Linear(hidden_dim, output_size)\n",
    "\n",
    "    def forward(self, X):\n",
    "        batch_size = X.size(0)\n",
    "        hidden = self.init_hidden(batch_size, X.device)       \n",
    "        out, hidden = self.rnn(X, hidden)\n",
    "        out = out.contiguous().view(-1, self.hidden_dim)\n",
    "        out = self.fully_connected(out)\n",
    "        return out, hidden\n",
    "        \n",
    "    def init_hidden(self, batch_size, device):\n",
    "        hidden = torch.zeros(self.n_layers, batch_size, self.hidden_dim, device=device)\n",
    "        return hidden\n",
    "    \n",
    "def predict_RNN(model, X):\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        device = next(model.parameters()).device\n",
    "        X = X.to(device)\n",
    "        predictions = model(X)\n",
    "    return predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training Loop Creation - very rough creation\n",
    "def train_RNN(model, X, y, learning_rate=0.1, n_epochs=100):\n",
    "    losses = []\n",
    "    criterion = nn.MSELoss()\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
    "    for epoch in range(n_epochs):\n",
    "        model.train()\n",
    "        optimizer.zero_grad()\n",
    "        output, hidden = model(X)\n",
    "        loss = criterion(output, y)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        if epoch % 10 == 0: \n",
    "            print(f'[{epoch}/{n_epochs}]',\n",
    "            f'Loss_D: {loss.item():.4f}')\n",
    "                 \n",
    "        losses.append(loss.item())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model Training\n",
    "Model_RNN = RNN()\n",
    "train_RNN(Model_RNN, train_X_tensor.unsqueeze(0), train_y_tensor) # Find out why I have to unsqueeze..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model Predictions\n",
    "y_pred_rnn, hidden_rnn = predict_RNN(Model_RNN, test_X_tensor.unsqueeze(0))\n",
    "y_pred_rnn = y_pred_rnn.detach()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model Evaluation\n",
    "mse_rnn = mean_squared_error(test_y, y_pred_rnn)\n",
    "mse_rnn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Visualization and Comparison\n",
    "https://neptune.ai/blog/deep-learning-visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10,10))\n",
    "plt.title('Random Forest Predictions')\n",
    "plt.xlabel('Real')\n",
    "plt.ylabel('Predicted')\n",
    "plt.scatter(test_y, y_pred_random_forest)\n",
    "plt.ylim((2.5,3.5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10,10))\n",
    "plt.title('Multilayer Perceptron Predictions')\n",
    "plt.xlabel('Real')\n",
    "plt.ylabel('Predicted')\n",
    "plt.scatter(test_y, y_pred_mlp)\n",
    "plt.ylim((2.5,4.5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10,10))\n",
    "plt.title('Recurrent Neural Network Predictions')\n",
    "plt.xlabel('Real')\n",
    "plt.ylabel('Predicted')\n",
    "plt.scatter(test_y, y_pred_rnn)\n",
    "plt.ylim((2.5,3.5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.style.use('dark_background')\n",
    "fig = plt.figure(figsize=(15, 15))\n",
    "ax = fig.add_subplot(111, projection='3d', facecolor='gray')\n",
    "scatter = ax.scatter(train_X[:29, 0], train_X[:29, 1], y_pred_rnn, c=y_pred_rnn, cmap='plasma', s=50, alpha=0.6)\n",
    "ax.set_xlabel('First Component', fontsize=14, color='white')\n",
    "ax.set_ylabel('Second Component', fontsize=14, color='white')\n",
    "ax.set_zlabel('Sensory Evaluation', fontsize=14, color='white')\n",
    "ax.set_title('3D Scatter Plot of Sensory Evaluation', fontsize=16, color='white')\n",
    "ax.tick_params(axis='x', colors='white')\n",
    "ax.tick_params(axis='y', colors='white')\n",
    "ax.tick_params(axis='z', colors='white')\n",
    "ax.view_init(elev=20, azim=120)\n",
    "cbar = fig.colorbar(scatter, ax=ax, pad=0.1)\n",
    "cbar.set_label('Sensory Evaluation Value', fontsize=14, color='white')\n",
    "cbar.ax.yaxis.set_tick_params(color='white')\n",
    "plt.setp(plt.getp(cbar.ax.axes, 'yticklabels'), color='white')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchviz import make_dot\n",
    "\n",
    "# Set model to evaluation mode and set it to current device\n",
    "Model_RNN.eval().to(device)\n",
    "\n",
    "# Create a dummy input tensor that matches the model's expected input dimensions\n",
    "dummy_input = torch.randn(1, 1, 2).to(device) \n",
    "\n",
    "# Perform a forward pass through the model with dummy input to get the output\n",
    "output, _ = Model_RNN(dummy_input.to(device))\n",
    "\n",
    "# Generate the visualization using the output tensor\n",
    "dot = make_dot(output, params=dict(Model_RNN.named_parameters()))\n",
    "\n",
    "# Render and save the visualization to a file\n",
    "dot.render(\"RNNModelVisualization\", format=\"png\")\n",
    "\n",
    "from IPython.display import Image\n",
    "\n",
    "Image('RNNModelVisualization.png')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Export model\n",
    "\n",
    "This model will be deployed in Python via a REST API with Flask. The Flask web app will then be deployed to a Linux web server using Unicorn."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_models():\n",
    "    torch.save(Model_Random_Forest, 'rf.pickle')\n",
    "    torch.save(Model_MLP, 'mlp.pickle')\n",
    "    torch.save(Model_RNN, 'rnn.pickle')\n",
    "\n",
    "save_models()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
