{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Standard Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import torch\n",
    "import torch.nn as nn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Constants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "SEED = 42\n",
    "\n",
    "np.random.seed(SEED)\n",
    "torch.manual_seed(SEED)\n",
    "torch.cuda.manual_seed(SEED)\n",
    "torch.backends.cudnn.deterministic = True\n",
    "\n",
    "catechins = ['Catechin', 'Epicatechin', 'Gallocatechin', 'Epigallocatechin',\n",
    "       'Catechin Gallate', 'Epicatechin Gallate', 'Gallocatechin Gallate',\n",
    "       'Epigallocatechin Gallate']\n",
    "chemicals = ['Catechin', 'Epicatechin', 'Gallocatechin', 'Epigallocatechin',\n",
    "       'Catechin Gallate', 'Epicatechin Gallate', 'Gallocatechin Gallate',\n",
    "       'Epigallocatechin Gallate', 'Caffeine']\n",
    "sensory_evaluations = ['Taste','Appearance','Aroma','Liqour color','Infused leaf']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Chemical and Instrumental Assessment of Green Tea Sensory Preference - Y.R. Liang, Q. Ye, J. Jin, H. Liang, J.L. Lu, Y.Y. Du & J.J. Dong\n",
    "# https://doi.org/10.1080/10942910701299430\n",
    "# Notes: downloaded CSVs\n",
    "\n",
    "df_10942910701299430 = pd.read_csv('../data/10942910701299430/chemical_composition.csv', header=1)\n",
    "df_10942910701299430 = pd.concat([df_10942910701299430, pd.read_csv('../data/10942910701299430/sensory_evaluation.csv')], axis=1)\n",
    "\n",
    "# Phytochemical Composition and Antioxidant Capacity of 30 Chinese Teas - by Guo-Yi Tang, Cai-Ning Zhao, Xiao-Yu Xu, Ren-You Gan, Shi-Yu Cao, Qing Liu, AoShang, Qian-Qian Mao & Hua-Bin Li\n",
    "# https://doi.org/10.3390/antiox8060180\n",
    "# Notes: Manually scraped data\n",
    "\n",
    "df_antiox8060180 = pd.read_csv('../data/antiox8060180/chemical_composition.csv')\n",
    "\n",
    "# Catechin and caffeine content of tea (Camellia sinensis L.) leaf significantly differ with seasonal variation... - Himangshu Deka, Tupu Barman, Jintu Dutta, Arundhuti Devi, Pradip Tamuly, Ranjit Kumar Paul & Tanmoy Karak \n",
    "# https://doi.org/10.1016/j.jfca.2020.103684\n",
    "# https://krishi.icar.gov.in/jspui/bitstream/123456789/68751/2/S0889157520313892-main.pdf - Free access\n",
    "\n",
    "# df_j.jfca.2020.103684_chemical_composition = pd.read_csv('dava/j.jfca.2020.103684/chemical_composition.csv')\n",
    "\n",
    "# Comparative analysis of tea catechins and theaflavins by high-performance liquid chromatography and capillary electrophoresis - Bee-Lan Lee & Choon-Nam Ong\n",
    "# https://doi.org/10.1016/S0021-9673(00)00215-6\n",
    "# Notes: Manually scraped data\n",
    "\n",
    "df_S0021967300002156 = pd.read_csv('../data/S0021967300002156/chemical_composition.csv')\n",
    "\n",
    "# Survey of Catechins, Gallic Acid, and Methylxanthines in Green, Oolong, Pu-erh, and Black Teas - Jen-Kun Lin, Chih-Li Lin, Yu-Chih Liang, Shoei-Yn Lin-Shiau & I-Ming Juan\n",
    "# https://doi.org/10.1021/jf980223x\n",
    "# Notes: Manually scraped data\n",
    "\n",
    "df_jf980223x = pd.read_csv('../data/jf980223x/chemical_composition.csv')\n",
    "\n",
    "# Analysis of some selected catechins and caffeine in green tea by high performance liquid chromatography\n",
    "# https://doi.org/10.1016/j.foodchem.2012.03.039\n",
    "# Notes: Manually scraped data\n",
    "\n",
    "df_jfoodchem201203039 = pd.read_csv('../data/jfoodchem201203039/chemical_composition.csv', on_bad_lines='skip')\n",
    "\n",
    "# Catechin content of 18 teas and a green tea extract supplement correlates with the antioxidant capacity \n",
    "# https://doi.org/10.1207/S15327914NC4502_13\n",
    "# Notes: Manually scraped data\n",
    "\n",
    "df_15327914NC4502_13 = pd.read_csv('../data/15327914NC4502_13/chemical_composition.csv')\n",
    "\n",
    "# Catechin contents in tea (Camellia sinensis) as affected by cultivar and environment and their relation to chlorophyll contents\n",
    "# https://doi.org/10.1016/j.foodchem.2010.08.029\n",
    "# Notes: Manually scraped data\n",
    "\n",
    "df_jfoodchem201008029 = pd.read_csv('../data/jfoodchem201008029/chemical_composition.csv')\n",
    "\n",
    "# Analysis of Catechin Content of Commercial Green Tea Products\n",
    "# https://doi.org/10.1080/J157v03n03_03\n",
    "# Notes: Manually scraped data\n",
    "\n",
    "df_J157v03n03_03 = pd.read_csv('../data/J157v03n03_03/chemical_composition.csv')\n",
    "\n",
    "# Phenol-Explorer database scraping\n",
    "# phenol-explorer.eu\n",
    "# Notes: Manually scraped data\n",
    "\n",
    "df_phenol_explorer = pd.read_csv('../data/phenol_explorer/chemical_composition.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# List of all imported dataframes\n",
    "all_dataframes = [df_10942910701299430,\n",
    "                  df_antiox8060180,\n",
    "                  df_S0021967300002156,\n",
    "                  df_jf980223x,\n",
    "                  df_jfoodchem201203039,\n",
    "                  df_15327914NC4502_13,\n",
    "                  df_phenol_explorer]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Combine data\n",
    "\n",
    "To combine the data I am going to create a data pipeline that does the following:\n",
    "\n",
    "1. Add all the pandas dataframes to a list.\n",
    "2. Read that list into a function.\n",
    "3. The function reads an individual dataframe from that list then resets the index, rename catechins to be like the catechin dictionary below, drop unused columns, add columns that are part of the standardized column format, and then sorts the columns to be part of the standardized column format.\n",
    "4. The function will then add the dataframe to a list denoting it is processed.\n",
    "5. Once the function has processed all the dataframes in the lists it will loop through the data frames in the finished list and concatenate them.\n",
    "6. The function will return a combined dataframe.\n",
    "\n",
    "## Data Frame Format\n",
    "\n",
    "The data frames will be combined into the following format:\n",
    "\n",
    "- `'Catechin'`\n",
    "- `'Epicatechin'`\n",
    "- `'Gallocatechin'`\n",
    "- `'Epigallocatechin'`\n",
    "- `'Catechin Gallate'`\n",
    "- `'Epicatechin Gallate'`\n",
    "- `'Gallocatechin Gallate'`\n",
    "- `'Epigallocatechin Gallate'`\n",
    "- `'Gallic Acid'`\n",
    "- `'Chlorogenic Acid'`\n",
    "- `'Caffeine'`\n",
    "- `'Taste'`\n",
    "- `'Appearance'`\n",
    "- `'Aroma'`\n",
    "- `'Liqour color'`\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "catechin_dictionary = {\n",
    "    'C': 'Catechin',\n",
    "    'EC': 'Epicatechin',\n",
    "    'GC': 'Gallocatechin',\n",
    "    'EGC': 'Epigallocatechin',\n",
    "    'CG': 'Catechin Gallate',\n",
    "    'ECG': 'Epicatechin Gallate',\n",
    "    'GCG': 'Gallocatechin Gallate',\n",
    "    'EGCG': 'Epigallocatechin Gallate'\n",
    "}\n",
    "\n",
    "standardized_columns = [\n",
    "    'Catechin',\n",
    "    'Epicatechin',\n",
    "    'Gallocatechin',\n",
    "    'Epigallocatechin',\n",
    "    'Catechin Gallate',\n",
    "    'Epicatechin Gallate',\n",
    "    'Gallocatechin Gallate',\n",
    "    'Epigallocatechin Gallate',\n",
    "    'Caffeine',\n",
    "    'Taste',\n",
    "    'Appearance',\n",
    "    'Aroma',\n",
    "    'Liqour color',\n",
    "    'Infused leaf'\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To faciliate scability I am create a sklearn pipeline and process all my data to be concatenated.\n",
    "First I will define the functions that will go into my data preprocessing pipeline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "def reset_index(dataframe: pd.DataFrame):\n",
    "    return dataframe.reset_index(drop=1)\n",
    "\n",
    "def rename_columns(dataframe: pd.DataFrame):\n",
    "    return dataframe.rename(columns=catechin_dictionary)\n",
    "\n",
    "def drop_nonstandardized_columns(dataframe: pd.DataFrame):\n",
    "    return dataframe.drop(columns=[column for column in dataframe if column not in standardized_columns])\n",
    "\n",
    "def reformat_columns(dataframe: pd.DataFrame):\n",
    "    return dataframe.reindex(columns=standardized_columns)\n",
    "\n",
    "def remove_standard_deviation_format(dataframe: pd.DataFrame):\n",
    "    for column in dataframe.columns:\n",
    "        if (dataframe[column].dtype.name == 'object') or (dataframe[column].dtype.name == 'string'):\n",
    "            dataframe[column] = dataframe[column].str.replace('Â± \\d*.\\d*', '', regex=True)\n",
    "    return dataframe    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, the data pipeline applies all of those functions to the inputed dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.preprocessing import FunctionTransformer\n",
    "\n",
    "ResetIndexTransformer = FunctionTransformer(reset_index)\n",
    "RenameColumnsTransformer = FunctionTransformer(rename_columns)\n",
    "DropNonstandardizedColumnsTransformer = FunctionTransformer(drop_nonstandardized_columns)\n",
    "ReformatColumnsTransformer = FunctionTransformer(reformat_columns)\n",
    "RemoveStandardDeviationFormat = FunctionTransformer(remove_standard_deviation_format)\n",
    "\n",
    "data_combining_pipeline = make_pipeline(\n",
    "    ResetIndexTransformer,\n",
    "    RenameColumnsTransformer,\n",
    "    DropNonstandardizedColumnsTransformer,\n",
    "    ReformatColumnsTransformer,\n",
    "    RemoveStandardDeviationFormat\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now I am creating a pipeline that transforms each dataframe then concatenates it and returns one dataframe to be used for the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_dataframes(dataframes: list):\n",
    "    processed_dataframes = []\n",
    "    for dataframe in dataframes:\n",
    "        dataframe = data_combining_pipeline.fit_transform(dataframe)\n",
    "        processed_dataframes.append(dataframe)\n",
    "    return processed_dataframes\n",
    "\n",
    "def concatenate_dataframes(dataframes: list):\n",
    "    formated_dataframe = pd.concat(dataframes, ignore_index=True)\n",
    "    return formated_dataframe\n",
    "\n",
    "def change_dtypes(dataframe: pd.DataFrame):\n",
    "    for column in dataframe.columns:\n",
    "        dataframe[column] = pd.to_numeric(dataframe[column], errors='coerce')\n",
    "    return dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "ProcessDataframes = FunctionTransformer(process_dataframes)\n",
    "ConcatenateDataframes = FunctionTransformer(concatenate_dataframes)\n",
    "Change_Dtypes = FunctionTransformer(change_dtypes)\n",
    "\n",
    "data_combining_pipeline_processor = make_pipeline(\n",
    "    ProcessDataframes,\n",
    "    ConcatenateDataframes,\n",
    "    Change_Dtypes\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Catechin</th>\n",
       "      <th>Epicatechin</th>\n",
       "      <th>Gallocatechin</th>\n",
       "      <th>Epigallocatechin</th>\n",
       "      <th>Catechin Gallate</th>\n",
       "      <th>Epicatechin Gallate</th>\n",
       "      <th>Gallocatechin Gallate</th>\n",
       "      <th>Epigallocatechin Gallate</th>\n",
       "      <th>Caffeine</th>\n",
       "      <th>Taste</th>\n",
       "      <th>Appearance</th>\n",
       "      <th>Aroma</th>\n",
       "      <th>Liqour color</th>\n",
       "      <th>Infused leaf</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>8.93</td>\n",
       "      <td>7.04</td>\n",
       "      <td>8.36</td>\n",
       "      <td>18.00</td>\n",
       "      <td>3.72</td>\n",
       "      <td>4.66</td>\n",
       "      <td>21.50</td>\n",
       "      <td>21.11</td>\n",
       "      <td>31.47</td>\n",
       "      <td>23.1</td>\n",
       "      <td>7.9</td>\n",
       "      <td>23.1</td>\n",
       "      <td>8.1</td>\n",
       "      <td>15.2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>9.87</td>\n",
       "      <td>NaN</td>\n",
       "      <td>22.55</td>\n",
       "      <td>10.92</td>\n",
       "      <td>5.33</td>\n",
       "      <td>5.29</td>\n",
       "      <td>26.62</td>\n",
       "      <td>23.42</td>\n",
       "      <td>39.24</td>\n",
       "      <td>25.8</td>\n",
       "      <td>8.1</td>\n",
       "      <td>24.9</td>\n",
       "      <td>8.3</td>\n",
       "      <td>16.4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>6.37</td>\n",
       "      <td>4.92</td>\n",
       "      <td>21.87</td>\n",
       "      <td>10.98</td>\n",
       "      <td>5.29</td>\n",
       "      <td>5.43</td>\n",
       "      <td>24.84</td>\n",
       "      <td>23.63</td>\n",
       "      <td>41.08</td>\n",
       "      <td>23.7</td>\n",
       "      <td>7.8</td>\n",
       "      <td>23.7</td>\n",
       "      <td>7.6</td>\n",
       "      <td>15.8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>6.78</td>\n",
       "      <td>5.91</td>\n",
       "      <td>23.08</td>\n",
       "      <td>11.47</td>\n",
       "      <td>5.76</td>\n",
       "      <td>6.43</td>\n",
       "      <td>25.34</td>\n",
       "      <td>22.14</td>\n",
       "      <td>50.97</td>\n",
       "      <td>23.7</td>\n",
       "      <td>8.3</td>\n",
       "      <td>24.0</td>\n",
       "      <td>8.6</td>\n",
       "      <td>16.6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5.82</td>\n",
       "      <td>5.60</td>\n",
       "      <td>24.12</td>\n",
       "      <td>13.35</td>\n",
       "      <td>5.28</td>\n",
       "      <td>5.94</td>\n",
       "      <td>27.92</td>\n",
       "      <td>25.09</td>\n",
       "      <td>53.70</td>\n",
       "      <td>24.6</td>\n",
       "      <td>8.2</td>\n",
       "      <td>26.4</td>\n",
       "      <td>8.5</td>\n",
       "      <td>16.6</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Catechin  Epicatechin  Gallocatechin  Epigallocatechin  Catechin Gallate  \\\n",
       "0      8.93         7.04           8.36             18.00              3.72   \n",
       "1      9.87          NaN          22.55             10.92              5.33   \n",
       "2      6.37         4.92          21.87             10.98              5.29   \n",
       "3      6.78         5.91          23.08             11.47              5.76   \n",
       "4      5.82         5.60          24.12             13.35              5.28   \n",
       "\n",
       "   Epicatechin Gallate  Gallocatechin Gallate  Epigallocatechin Gallate  \\\n",
       "0                 4.66                  21.50                     21.11   \n",
       "1                 5.29                  26.62                     23.42   \n",
       "2                 5.43                  24.84                     23.63   \n",
       "3                 6.43                  25.34                     22.14   \n",
       "4                 5.94                  27.92                     25.09   \n",
       "\n",
       "   Caffeine  Taste  Appearance  Aroma  Liqour color  Infused leaf  \n",
       "0     31.47   23.1         7.9   23.1           8.1          15.2  \n",
       "1     39.24   25.8         8.1   24.9           8.3          16.4  \n",
       "2     41.08   23.7         7.8   23.7           7.6          15.8  \n",
       "3     50.97   23.7         8.3   24.0           8.6          16.6  \n",
       "4     53.70   24.6         8.2   26.4           8.5          16.6  "
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = data_combining_pipeline_processor.transform(all_dataframes)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Remove Extraneous Data\n",
    "It is common for this aggregated dataset to come with columns that contain no data at all, that will be removed here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.dropna(how='all', axis=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Explanation\n",
    "\n",
    "The data we will be examining and training on is purely numerical. This has many advantages in terms of cleaning and training but a few disadvantages that may not be readily appearant. The catechins and caffeine are all listed in milligrams. What is taste, appearance, aroma, and liquor color measured in? Milligrams? No, instead it is much more complicated. Thankfully this is a pretty standardized tea tasting scoring system throughout China so most of our data will reflect that, any data that does not reflect that will be modified into this scoring system. The scoring system is outlined below:\n",
    "\n",
    "\n",
    "    Scoring System: They used a scoring system where the highest possible score for a tea sample was 100. This score was divided into different categories, each representing a different aspect of the tea:\n",
    "        Dry Tea Appearance (10% of the total score): This is about how the tea leaves look when they are dry. The experts looked at the color, size, how tender the leaves were, and how evenly the leaves were twisted.\n",
    "        Aroma (30% of the total score): This is about how the tea smells.\n",
    "        Liquor Color (10% of the total score): This is about the color of the tea when it's steeped in water.\n",
    "        Taste (30% of the total score): This is, of course, about how the tea tastes.\n",
    "        Infused Leaf (20% of the total score): After the tea has been steeped, the experts looked at the leaves again to see their color, size, tenderness, and evenness.\n",
    "\n",
    "    How They Tested:\n",
    "        Dry Tea Appearance: They placed 200 grams of dry tea on a wooden tray (20 cm x 20 cm) and scored it based on the criteria mentioned above.\n",
    "        Preparing the Tea: For testing aroma, liquor color, and taste, they steeped 3 grams of the tea in 150 milliliters of boiling water for 5 minutes.\n",
    "        Tasting and Scoring: The tea was then poured into a special tasting bowl. The experts compared the aroma, color, and taste of the tea to three reference samples they had in their lab to give their scores.\n",
    "        Infused Leaf: Finally, they put the steeped leaves into a porcelain tray with cold water and scored them based on color, size, tenderness, and evenness.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exploratory Data Analysis\n",
    "\n",
    "This pairplot offers a high-level visualization of what is going on internally in the dataset. Next, we will examine further the intricacies of the data before cleaning it further and imputing data using a Generative Adversarial Network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<seaborn.axisgrid.PairGrid at 0x7dc8c0eb2290>"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sns.set_theme()\n",
    "sns.pairplot(df[catechins])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Summary Statistics\n",
    "\n",
    "The summary statistics of the agggregated data is displayed below. It is interesting to note that there are some significant outliers. One such outlier is Epigallocatechin with a max of aroun 100mg."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Catechin</th>\n",
       "      <th>Epicatechin</th>\n",
       "      <th>Gallocatechin</th>\n",
       "      <th>Epigallocatechin</th>\n",
       "      <th>Catechin Gallate</th>\n",
       "      <th>Epicatechin Gallate</th>\n",
       "      <th>Gallocatechin Gallate</th>\n",
       "      <th>Epigallocatechin Gallate</th>\n",
       "      <th>Caffeine</th>\n",
       "      <th>Taste</th>\n",
       "      <th>Appearance</th>\n",
       "      <th>Aroma</th>\n",
       "      <th>Liqour color</th>\n",
       "      <th>Infused leaf</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>65.000000</td>\n",
       "      <td>83.000000</td>\n",
       "      <td>33.000000</td>\n",
       "      <td>85.000000</td>\n",
       "      <td>34.000000</td>\n",
       "      <td>84.000000</td>\n",
       "      <td>53.000000</td>\n",
       "      <td>85.000000</td>\n",
       "      <td>77.000000</td>\n",
       "      <td>24.000000</td>\n",
       "      <td>24.000000</td>\n",
       "      <td>24.000000</td>\n",
       "      <td>24.000000</td>\n",
       "      <td>24.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>4.078031</td>\n",
       "      <td>5.307024</td>\n",
       "      <td>17.353697</td>\n",
       "      <td>16.613271</td>\n",
       "      <td>6.136618</td>\n",
       "      <td>8.397976</td>\n",
       "      <td>16.685358</td>\n",
       "      <td>29.376024</td>\n",
       "      <td>22.670130</td>\n",
       "      <td>24.275000</td>\n",
       "      <td>7.937500</td>\n",
       "      <td>24.545833</td>\n",
       "      <td>8.241667</td>\n",
       "      <td>16.433333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>3.806904</td>\n",
       "      <td>3.874804</td>\n",
       "      <td>9.884853</td>\n",
       "      <td>18.974582</td>\n",
       "      <td>4.274245</td>\n",
       "      <td>9.617208</td>\n",
       "      <td>23.340435</td>\n",
       "      <td>34.036584</td>\n",
       "      <td>20.182393</td>\n",
       "      <td>1.506291</td>\n",
       "      <td>0.436201</td>\n",
       "      <td>1.601352</td>\n",
       "      <td>0.461488</td>\n",
       "      <td>0.856010</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>0.020000</td>\n",
       "      <td>0.190000</td>\n",
       "      <td>1.824000</td>\n",
       "      <td>0.196000</td>\n",
       "      <td>0.300000</td>\n",
       "      <td>0.070000</td>\n",
       "      <td>0.160000</td>\n",
       "      <td>0.300000</td>\n",
       "      <td>0.558000</td>\n",
       "      <td>22.200000</td>\n",
       "      <td>6.800000</td>\n",
       "      <td>22.200000</td>\n",
       "      <td>7.300000</td>\n",
       "      <td>14.800000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>1.190000</td>\n",
       "      <td>1.770000</td>\n",
       "      <td>4.002000</td>\n",
       "      <td>3.120000</td>\n",
       "      <td>4.445000</td>\n",
       "      <td>3.310000</td>\n",
       "      <td>0.630000</td>\n",
       "      <td>13.610000</td>\n",
       "      <td>1.970000</td>\n",
       "      <td>23.325000</td>\n",
       "      <td>7.800000</td>\n",
       "      <td>23.325000</td>\n",
       "      <td>7.950000</td>\n",
       "      <td>15.800000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>2.580000</td>\n",
       "      <td>5.270000</td>\n",
       "      <td>21.870000</td>\n",
       "      <td>12.180000</td>\n",
       "      <td>5.755000</td>\n",
       "      <td>5.975000</td>\n",
       "      <td>6.200000</td>\n",
       "      <td>23.630000</td>\n",
       "      <td>16.300000</td>\n",
       "      <td>23.850000</td>\n",
       "      <td>7.950000</td>\n",
       "      <td>24.400000</td>\n",
       "      <td>8.300000</td>\n",
       "      <td>16.400000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>6.130000</td>\n",
       "      <td>6.955000</td>\n",
       "      <td>23.900000</td>\n",
       "      <td>22.000000</td>\n",
       "      <td>7.195000</td>\n",
       "      <td>10.177500</td>\n",
       "      <td>27.050000</td>\n",
       "      <td>33.102000</td>\n",
       "      <td>41.080000</td>\n",
       "      <td>25.275000</td>\n",
       "      <td>8.125000</td>\n",
       "      <td>25.875000</td>\n",
       "      <td>8.525000</td>\n",
       "      <td>16.900000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>16.540000</td>\n",
       "      <td>19.700000</td>\n",
       "      <td>32.020000</td>\n",
       "      <td>100.684000</td>\n",
       "      <td>22.000000</td>\n",
       "      <td>72.000000</td>\n",
       "      <td>150.900000</td>\n",
       "      <td>285.100000</td>\n",
       "      <td>58.830000</td>\n",
       "      <td>27.600000</td>\n",
       "      <td>8.800000</td>\n",
       "      <td>27.600000</td>\n",
       "      <td>8.900000</td>\n",
       "      <td>17.800000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        Catechin  Epicatechin  Gallocatechin  Epigallocatechin  \\\n",
       "count  65.000000    83.000000      33.000000         85.000000   \n",
       "mean    4.078031     5.307024      17.353697         16.613271   \n",
       "std     3.806904     3.874804       9.884853         18.974582   \n",
       "min     0.020000     0.190000       1.824000          0.196000   \n",
       "25%     1.190000     1.770000       4.002000          3.120000   \n",
       "50%     2.580000     5.270000      21.870000         12.180000   \n",
       "75%     6.130000     6.955000      23.900000         22.000000   \n",
       "max    16.540000    19.700000      32.020000        100.684000   \n",
       "\n",
       "       Catechin Gallate  Epicatechin Gallate  Gallocatechin Gallate  \\\n",
       "count         34.000000            84.000000              53.000000   \n",
       "mean           6.136618             8.397976              16.685358   \n",
       "std            4.274245             9.617208              23.340435   \n",
       "min            0.300000             0.070000               0.160000   \n",
       "25%            4.445000             3.310000               0.630000   \n",
       "50%            5.755000             5.975000               6.200000   \n",
       "75%            7.195000            10.177500              27.050000   \n",
       "max           22.000000            72.000000             150.900000   \n",
       "\n",
       "       Epigallocatechin Gallate   Caffeine      Taste  Appearance      Aroma  \\\n",
       "count                 85.000000  77.000000  24.000000   24.000000  24.000000   \n",
       "mean                  29.376024  22.670130  24.275000    7.937500  24.545833   \n",
       "std                   34.036584  20.182393   1.506291    0.436201   1.601352   \n",
       "min                    0.300000   0.558000  22.200000    6.800000  22.200000   \n",
       "25%                   13.610000   1.970000  23.325000    7.800000  23.325000   \n",
       "50%                   23.630000  16.300000  23.850000    7.950000  24.400000   \n",
       "75%                   33.102000  41.080000  25.275000    8.125000  25.875000   \n",
       "max                  285.100000  58.830000  27.600000    8.800000  27.600000   \n",
       "\n",
       "       Liqour color  Infused leaf  \n",
       "count     24.000000     24.000000  \n",
       "mean       8.241667     16.433333  \n",
       "std        0.461488      0.856010  \n",
       "min        7.300000     14.800000  \n",
       "25%        7.950000     15.800000  \n",
       "50%        8.300000     16.400000  \n",
       "75%        8.525000     16.900000  \n",
       "max        8.900000     17.800000  "
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training Device\n",
    "\n",
    "As I am developing on Linux and training on Windows, this code displays if CUDA functionality is being used or not."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cpu device\n"
     ]
    }
   ],
   "source": [
    "# Get cpu, gpu or mps device for training.\n",
    "device = (\n",
    "    \"cuda\"\n",
    "    if torch.cuda.is_available()\n",
    "    else \"mps\"\n",
    "    if torch.backends.mps.is_available()\n",
    "    else \"cpu\"\n",
    ")\n",
    "print(f\"Using {device} device\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Split data\n",
    "\n",
    "For this model we will not be using a validation set because of the limited nature of the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from torch.utils.data import DataLoader, random_split\n",
    "\n",
    "# train_size = int(0.8 * len(df))\n",
    "# test_size = len(df) - train_size\n",
    "# train_df, test_df = random_split(df, [train_size, test_size])\n",
    "\n",
    "# train_loader = DataLoader(train_df, batch_size=64, shuffle=True)\n",
    "# test_loader = DataLoader(test_df, batch_size=64, shuffle=True)\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "train_df, test_df = train_test_split(df, test_size=0.2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Imputation and Scaling\n",
    "\n",
    "The data imputation for this aggregated dataset will be done in two parts:\n",
    "\n",
    "1. **Chemicals Imputation**:\n",
    "   - The chemicals will be imputed using sklearn's `SimpleImputer`.\n",
    "\n",
    "2. **Min-Max Scaler**:\n",
    "   - A min-max scaler will be applied so the GAN can learn features correctly.\n",
    "\n",
    "3. **Sensory Appeals Generation**:\n",
    "   - The sensory appeals will be generated by a custom `Generative Adversarial Network (GAN)`.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Missing Chemical Imputation with an Iterative Imputer\n",
    "Note this is causing some negative values that need to be fixed, possibly rows with complete missing data is causing this."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/tahlon/Documents/Github/DTSC-691-Capstone-Project/.venv/lib/python3.11/site-packages/sklearn/impute/_iterative.py:801: ConvergenceWarning: [IterativeImputer] Early stopping criterion not reached.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Catechin</th>\n",
       "      <th>Epicatechin</th>\n",
       "      <th>Gallocatechin</th>\n",
       "      <th>Epigallocatechin</th>\n",
       "      <th>Catechin Gallate</th>\n",
       "      <th>Epicatechin Gallate</th>\n",
       "      <th>Gallocatechin Gallate</th>\n",
       "      <th>Epigallocatechin Gallate</th>\n",
       "      <th>Caffeine</th>\n",
       "      <th>Taste</th>\n",
       "      <th>Appearance</th>\n",
       "      <th>Aroma</th>\n",
       "      <th>Liqour color</th>\n",
       "      <th>Infused leaf</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>80</th>\n",
       "      <td>2.210249</td>\n",
       "      <td>7.000</td>\n",
       "      <td>2.391918</td>\n",
       "      <td>38.700</td>\n",
       "      <td>0.300000</td>\n",
       "      <td>9.500000</td>\n",
       "      <td>3.100000</td>\n",
       "      <td>49.80</td>\n",
       "      <td>21.800000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>8.930000</td>\n",
       "      <td>7.040</td>\n",
       "      <td>8.360000</td>\n",
       "      <td>18.000</td>\n",
       "      <td>3.720000</td>\n",
       "      <td>4.660000</td>\n",
       "      <td>21.500000</td>\n",
       "      <td>21.11</td>\n",
       "      <td>31.470000</td>\n",
       "      <td>23.1</td>\n",
       "      <td>7.9</td>\n",
       "      <td>23.1</td>\n",
       "      <td>8.1</td>\n",
       "      <td>15.2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>70</th>\n",
       "      <td>0.985000</td>\n",
       "      <td>0.644</td>\n",
       "      <td>6.852867</td>\n",
       "      <td>2.820</td>\n",
       "      <td>-3.058173</td>\n",
       "      <td>-0.237467</td>\n",
       "      <td>-6.379478</td>\n",
       "      <td>1.91</td>\n",
       "      <td>0.750000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>7.530000</td>\n",
       "      <td>6.910</td>\n",
       "      <td>24.840000</td>\n",
       "      <td>12.870</td>\n",
       "      <td>5.800000</td>\n",
       "      <td>6.010000</td>\n",
       "      <td>30.740000</td>\n",
       "      <td>29.52</td>\n",
       "      <td>48.210000</td>\n",
       "      <td>24.0</td>\n",
       "      <td>7.9</td>\n",
       "      <td>23.7</td>\n",
       "      <td>7.8</td>\n",
       "      <td>16.4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>6.050000</td>\n",
       "      <td>5.800</td>\n",
       "      <td>19.260000</td>\n",
       "      <td>10.440</td>\n",
       "      <td>6.210000</td>\n",
       "      <td>7.430000</td>\n",
       "      <td>32.120000</td>\n",
       "      <td>31.44</td>\n",
       "      <td>52.860000</td>\n",
       "      <td>25.8</td>\n",
       "      <td>8.1</td>\n",
       "      <td>25.8</td>\n",
       "      <td>8.5</td>\n",
       "      <td>17.6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>84</th>\n",
       "      <td>2.307887</td>\n",
       "      <td>2.900</td>\n",
       "      <td>7.472179</td>\n",
       "      <td>22.200</td>\n",
       "      <td>0.900000</td>\n",
       "      <td>5.200000</td>\n",
       "      <td>3.400000</td>\n",
       "      <td>37.70</td>\n",
       "      <td>0.700000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>6.130000</td>\n",
       "      <td>6.310</td>\n",
       "      <td>23.900000</td>\n",
       "      <td>12.410</td>\n",
       "      <td>4.330000</td>\n",
       "      <td>5.940000</td>\n",
       "      <td>26.070000</td>\n",
       "      <td>26.00</td>\n",
       "      <td>44.210000</td>\n",
       "      <td>22.5</td>\n",
       "      <td>7.8</td>\n",
       "      <td>22.8</td>\n",
       "      <td>7.5</td>\n",
       "      <td>15.6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>11.770000</td>\n",
       "      <td>8.480</td>\n",
       "      <td>32.020000</td>\n",
       "      <td>13.450</td>\n",
       "      <td>15.150000</td>\n",
       "      <td>18.720000</td>\n",
       "      <td>31.710000</td>\n",
       "      <td>34.36</td>\n",
       "      <td>56.710000</td>\n",
       "      <td>27.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>26.7</td>\n",
       "      <td>8.9</td>\n",
       "      <td>17.6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5.820000</td>\n",
       "      <td>5.600</td>\n",
       "      <td>24.120000</td>\n",
       "      <td>13.350</td>\n",
       "      <td>5.280000</td>\n",
       "      <td>5.940000</td>\n",
       "      <td>27.920000</td>\n",
       "      <td>25.09</td>\n",
       "      <td>53.700000</td>\n",
       "      <td>24.6</td>\n",
       "      <td>8.2</td>\n",
       "      <td>26.4</td>\n",
       "      <td>8.5</td>\n",
       "      <td>16.6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>68</th>\n",
       "      <td>0.734000</td>\n",
       "      <td>1.310</td>\n",
       "      <td>5.184318</td>\n",
       "      <td>5.230</td>\n",
       "      <td>-3.296350</td>\n",
       "      <td>0.387000</td>\n",
       "      <td>-8.734864</td>\n",
       "      <td>3.56</td>\n",
       "      <td>0.558000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42</th>\n",
       "      <td>2.365584</td>\n",
       "      <td>1.410</td>\n",
       "      <td>11.435441</td>\n",
       "      <td>2.840</td>\n",
       "      <td>-0.765263</td>\n",
       "      <td>6.820000</td>\n",
       "      <td>3.799864</td>\n",
       "      <td>5.52</td>\n",
       "      <td>22.609437</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49</th>\n",
       "      <td>0.020000</td>\n",
       "      <td>0.700</td>\n",
       "      <td>8.284801</td>\n",
       "      <td>0.700</td>\n",
       "      <td>-2.324323</td>\n",
       "      <td>1.390000</td>\n",
       "      <td>0.580000</td>\n",
       "      <td>10.99</td>\n",
       "      <td>6.940000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37</th>\n",
       "      <td>4.969755</td>\n",
       "      <td>6.060</td>\n",
       "      <td>11.478311</td>\n",
       "      <td>23.460</td>\n",
       "      <td>2.490434</td>\n",
       "      <td>12.660000</td>\n",
       "      <td>9.259951</td>\n",
       "      <td>29.83</td>\n",
       "      <td>31.658758</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>74</th>\n",
       "      <td>2.820000</td>\n",
       "      <td>5.100</td>\n",
       "      <td>5.542858</td>\n",
       "      <td>20.000</td>\n",
       "      <td>-0.885217</td>\n",
       "      <td>4.220000</td>\n",
       "      <td>-4.288304</td>\n",
       "      <td>24.20</td>\n",
       "      <td>2.140000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>57</th>\n",
       "      <td>1.900000</td>\n",
       "      <td>5.080</td>\n",
       "      <td>4.803502</td>\n",
       "      <td>0.877</td>\n",
       "      <td>-3.347486</td>\n",
       "      <td>6.630000</td>\n",
       "      <td>-12.663508</td>\n",
       "      <td>4.05</td>\n",
       "      <td>1.440000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>56</th>\n",
       "      <td>2.480000</td>\n",
       "      <td>2.620</td>\n",
       "      <td>9.074238</td>\n",
       "      <td>1.230</td>\n",
       "      <td>-2.171405</td>\n",
       "      <td>4.980000</td>\n",
       "      <td>-3.069208</td>\n",
       "      <td>7.77</td>\n",
       "      <td>1.890000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>5.184446</td>\n",
       "      <td>6.060</td>\n",
       "      <td>7.656033</td>\n",
       "      <td>36.530</td>\n",
       "      <td>2.410099</td>\n",
       "      <td>5.340000</td>\n",
       "      <td>-0.194918</td>\n",
       "      <td>18.10</td>\n",
       "      <td>33.715453</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     Catechin  Epicatechin  Gallocatechin  Epigallocatechin  Catechin Gallate  \\\n",
       "80   2.210249        7.000       2.391918            38.700          0.300000   \n",
       "0    8.930000        7.040       8.360000            18.000          3.720000   \n",
       "70   0.985000        0.644       6.852867             2.820         -3.058173   \n",
       "22   7.530000        6.910      24.840000            12.870          5.800000   \n",
       "12   6.050000        5.800      19.260000            10.440          6.210000   \n",
       "84   2.307887        2.900       7.472179            22.200          0.900000   \n",
       "10   6.130000        6.310      23.900000            12.410          4.330000   \n",
       "18  11.770000        8.480      32.020000            13.450         15.150000   \n",
       "4    5.820000        5.600      24.120000            13.350          5.280000   \n",
       "68   0.734000        1.310       5.184318             5.230         -3.296350   \n",
       "42   2.365584        1.410      11.435441             2.840         -0.765263   \n",
       "49   0.020000        0.700       8.284801             0.700         -2.324323   \n",
       "37   4.969755        6.060      11.478311            23.460          2.490434   \n",
       "74   2.820000        5.100       5.542858            20.000         -0.885217   \n",
       "57   1.900000        5.080       4.803502             0.877         -3.347486   \n",
       "56   2.480000        2.620       9.074238             1.230         -2.171405   \n",
       "35   5.184446        6.060       7.656033            36.530          2.410099   \n",
       "\n",
       "    Epicatechin Gallate  Gallocatechin Gallate  Epigallocatechin Gallate  \\\n",
       "80             9.500000               3.100000                     49.80   \n",
       "0              4.660000              21.500000                     21.11   \n",
       "70            -0.237467              -6.379478                      1.91   \n",
       "22             6.010000              30.740000                     29.52   \n",
       "12             7.430000              32.120000                     31.44   \n",
       "84             5.200000               3.400000                     37.70   \n",
       "10             5.940000              26.070000                     26.00   \n",
       "18            18.720000              31.710000                     34.36   \n",
       "4              5.940000              27.920000                     25.09   \n",
       "68             0.387000              -8.734864                      3.56   \n",
       "42             6.820000               3.799864                      5.52   \n",
       "49             1.390000               0.580000                     10.99   \n",
       "37            12.660000               9.259951                     29.83   \n",
       "74             4.220000              -4.288304                     24.20   \n",
       "57             6.630000             -12.663508                      4.05   \n",
       "56             4.980000              -3.069208                      7.77   \n",
       "35             5.340000              -0.194918                     18.10   \n",
       "\n",
       "     Caffeine  Taste  Appearance  Aroma  Liqour color  Infused leaf  \n",
       "80  21.800000    NaN         NaN    NaN           NaN           NaN  \n",
       "0   31.470000   23.1         7.9   23.1           8.1          15.2  \n",
       "70   0.750000    NaN         NaN    NaN           NaN           NaN  \n",
       "22  48.210000   24.0         7.9   23.7           7.8          16.4  \n",
       "12  52.860000   25.8         8.1   25.8           8.5          17.6  \n",
       "84   0.700000    NaN         NaN    NaN           NaN           NaN  \n",
       "10  44.210000   22.5         7.8   22.8           7.5          15.6  \n",
       "18  56.710000   27.0         8.0   26.7           8.9          17.6  \n",
       "4   53.700000   24.6         8.2   26.4           8.5          16.6  \n",
       "68   0.558000    NaN         NaN    NaN           NaN           NaN  \n",
       "42  22.609437    NaN         NaN    NaN           NaN           NaN  \n",
       "49   6.940000    NaN         NaN    NaN           NaN           NaN  \n",
       "37  31.658758    NaN         NaN    NaN           NaN           NaN  \n",
       "74   2.140000    NaN         NaN    NaN           NaN           NaN  \n",
       "57   1.440000    NaN         NaN    NaN           NaN           NaN  \n",
       "56   1.890000    NaN         NaN    NaN           NaN           NaN  \n",
       "35  33.715453    NaN         NaN    NaN           NaN           NaN  "
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.experimental import enable_iterative_imputer\n",
    "from sklearn.impute import IterativeImputer\n",
    "\n",
    "imputer = IterativeImputer()\n",
    "\n",
    "train_df[chemicals] = imputer.fit_transform(train_df[chemicals])\n",
    "test_df[chemicals] = imputer.transform(test_df[chemicals])\n",
    "\n",
    "test_df\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature Scaling\n",
    "The data is purely numerical, therefore the data will be only normalized. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Normalization: Min-Max Scaler\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "scaler = MinMaxScaler() # Note: I am still not sure if I want to scale the labels.\n",
    "\n",
    "scaled_train_df = scaler.fit_transform(train_df)\n",
    "scaled_test_df = scaler.transform(test_df)\n",
    "\n",
    "scaled_train_df = pd.DataFrame(scaled_train_df, columns=standardized_columns)\n",
    "scaled_test_df = pd.DataFrame(scaled_test_df, columns=standardized_columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Catechin</th>\n",
       "      <th>Epicatechin</th>\n",
       "      <th>Gallocatechin</th>\n",
       "      <th>Epigallocatechin</th>\n",
       "      <th>Catechin Gallate</th>\n",
       "      <th>Epicatechin Gallate</th>\n",
       "      <th>Gallocatechin Gallate</th>\n",
       "      <th>Epigallocatechin Gallate</th>\n",
       "      <th>Caffeine</th>\n",
       "      <th>Taste</th>\n",
       "      <th>Appearance</th>\n",
       "      <th>Aroma</th>\n",
       "      <th>Liqour color</th>\n",
       "      <th>Infused leaf</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.216707</td>\n",
       "      <td>0.779600</td>\n",
       "      <td>0.103659</td>\n",
       "      <td>0.487660</td>\n",
       "      <td>0.223978</td>\n",
       "      <td>0.220075</td>\n",
       "      <td>0.109277</td>\n",
       "      <td>0.227177</td>\n",
       "      <td>0.494321</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.019477</td>\n",
       "      <td>0.197515</td>\n",
       "      <td>0.005215</td>\n",
       "      <td>0.038169</td>\n",
       "      <td>0.016127</td>\n",
       "      <td>0.084487</td>\n",
       "      <td>0.046735</td>\n",
       "      <td>0.109263</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.044593</td>\n",
       "      <td>0.198501</td>\n",
       "      <td>0.009693</td>\n",
       "      <td>0.038713</td>\n",
       "      <td>0.024607</td>\n",
       "      <td>0.084730</td>\n",
       "      <td>0.047296</td>\n",
       "      <td>0.104624</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.360169</td>\n",
       "      <td>0.235264</td>\n",
       "      <td>0.428724</td>\n",
       "      <td>0.122642</td>\n",
       "      <td>0.341569</td>\n",
       "      <td>0.087724</td>\n",
       "      <td>0.238875</td>\n",
       "      <td>0.090414</td>\n",
       "      <td>0.740202</td>\n",
       "      <td>0.388889</td>\n",
       "      <td>0.65</td>\n",
       "      <td>0.722222</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.600000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.436250</td>\n",
       "      <td>0.430036</td>\n",
       "      <td>0.119932</td>\n",
       "      <td>0.736566</td>\n",
       "      <td>0.293526</td>\n",
       "      <td>0.155624</td>\n",
       "      <td>0.087039</td>\n",
       "      <td>0.157008</td>\n",
       "      <td>0.495919</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>63</th>\n",
       "      <td>0.884383</td>\n",
       "      <td>0.842132</td>\n",
       "      <td>0.443768</td>\n",
       "      <td>0.123935</td>\n",
       "      <td>0.420755</td>\n",
       "      <td>0.208536</td>\n",
       "      <td>0.136497</td>\n",
       "      <td>0.043820</td>\n",
       "      <td>0.555147</td>\n",
       "      <td>0.055556</td>\n",
       "      <td>0.15</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>64</th>\n",
       "      <td>0.093826</td>\n",
       "      <td>0.120964</td>\n",
       "      <td>0.185344</td>\n",
       "      <td>0.002558</td>\n",
       "      <td>0.012210</td>\n",
       "      <td>0.045739</td>\n",
       "      <td>0.039886</td>\n",
       "      <td>0.013904</td>\n",
       "      <td>0.009777</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>65</th>\n",
       "      <td>0.136804</td>\n",
       "      <td>0.111225</td>\n",
       "      <td>0.210562</td>\n",
       "      <td>0.072884</td>\n",
       "      <td>0.063158</td>\n",
       "      <td>0.045600</td>\n",
       "      <td>0.074808</td>\n",
       "      <td>0.047753</td>\n",
       "      <td>0.018883</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>66</th>\n",
       "      <td>0.366223</td>\n",
       "      <td>0.319836</td>\n",
       "      <td>0.443091</td>\n",
       "      <td>0.121447</td>\n",
       "      <td>0.383142</td>\n",
       "      <td>0.104824</td>\n",
       "      <td>0.255827</td>\n",
       "      <td>0.104846</td>\n",
       "      <td>0.791921</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.85</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.866667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>67</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.037417</td>\n",
       "      <td>0.195971</td>\n",
       "      <td>0.007503</td>\n",
       "      <td>0.038640</td>\n",
       "      <td>0.027388</td>\n",
       "      <td>0.085155</td>\n",
       "      <td>0.051194</td>\n",
       "      <td>0.107717</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>68 rows Ã 14 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "    Catechin  Epicatechin  Gallocatechin  Epigallocatechin  Catechin Gallate  \\\n",
       "0   0.216707     0.779600       0.103659          0.487660          0.223978   \n",
       "1   0.000000     0.019477       0.197515          0.005215          0.038169   \n",
       "2   0.000000     0.044593       0.198501          0.009693          0.038713   \n",
       "3   0.360169     0.235264       0.428724          0.122642          0.341569   \n",
       "4   0.436250     0.430036       0.119932          0.736566          0.293526   \n",
       "..       ...          ...            ...               ...               ...   \n",
       "63  0.884383     0.842132       0.443768          0.123935          0.420755   \n",
       "64  0.093826     0.120964       0.185344          0.002558          0.012210   \n",
       "65  0.136804     0.111225       0.210562          0.072884          0.063158   \n",
       "66  0.366223     0.319836       0.443091          0.121447          0.383142   \n",
       "67  0.000000     0.037417       0.195971          0.007503          0.038640   \n",
       "\n",
       "    Epicatechin Gallate  Gallocatechin Gallate  Epigallocatechin Gallate  \\\n",
       "0              0.220075               0.109277                  0.227177   \n",
       "1              0.016127               0.084487                  0.046735   \n",
       "2              0.024607               0.084730                  0.047296   \n",
       "3              0.087724               0.238875                  0.090414   \n",
       "4              0.155624               0.087039                  0.157008   \n",
       "..                  ...                    ...                       ...   \n",
       "63             0.208536               0.136497                  0.043820   \n",
       "64             0.045739               0.039886                  0.013904   \n",
       "65             0.045600               0.074808                  0.047753   \n",
       "66             0.104824               0.255827                  0.104846   \n",
       "67             0.027388               0.085155                  0.051194   \n",
       "\n",
       "    Caffeine     Taste  Appearance     Aroma  Liqour color  Infused leaf  \n",
       "0   0.494321       NaN         NaN       NaN           NaN           NaN  \n",
       "1   0.109263       NaN         NaN       NaN           NaN           NaN  \n",
       "2   0.104624       NaN         NaN       NaN           NaN           NaN  \n",
       "3   0.740202  0.388889        0.65  0.722222           1.0      0.600000  \n",
       "4   0.495919       NaN         NaN       NaN           NaN           NaN  \n",
       "..       ...       ...         ...       ...           ...           ...  \n",
       "63  0.555147  0.055556        0.15  0.000000           0.0      0.000000  \n",
       "64  0.009777       NaN         NaN       NaN           NaN           NaN  \n",
       "65  0.018883       NaN         NaN       NaN           NaN           NaN  \n",
       "66  0.791921  1.000000        0.85  1.000000           1.0      0.866667  \n",
       "67  0.107717       NaN         NaN       NaN           NaN           NaN  \n",
       "\n",
       "[68 rows x 14 columns]"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scaled_train_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generative Adversarial Network\n",
    "\n",
    "The purpose of the GAN is to impute missing sensory values. A GAN is a combination of two neural networks: a generator and a discriminator. The generator will generate numbers that it thinks are the appropiate sensory evaluations based on the givens catechins and caffeine input. The discriminator will then decide if that is a realistic output or not. In the end, all missing data on sensory evaluations will be naturally imputted. Much of this was interpreted from https://pytorch.org/tutorials/beginner/dcgan_faces_tutorial.html although specifically using GAN not DCGAN."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a training dataset for the GAN (rows that include all sensory scores)\n",
    "GAN_train_df = scaled_train_df[(scaled_train_df[sensory_evaluations] > 0).all(axis=1)].dropna(axis=0) # Subset of data containinag complete sensory evaluations\n",
    "GAN_test_df = scaled_test_df[(scaled_test_df[sensory_evaluations] > 0).all(axis=1)].dropna(axis=0) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Generator(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Generator, self).__init__()\n",
    "        self.main = nn.Sequential(\n",
    "            # Input Layer\n",
    "            nn.Linear(9, 128),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "\n",
    "            nn.Linear(128, 256),\n",
    "            nn.BatchNorm1d(256),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "\n",
    "            nn.Linear(256, 128),\n",
    "            nn.BatchNorm1d(128),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "\n",
    "            nn.Linear(128, 5),\n",
    "            nn.Sigmoid() # This is used because output values are in the range [0, 1)\n",
    "        )\n",
    "\n",
    "\n",
    "    def forward(self, input):\n",
    "        output = self.main(input)\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Discriminator(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Discriminator, self).__init__()\n",
    "        self.main = nn.Sequential(\n",
    "            # Input layer\n",
    "            nn.Linear(14, 16),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "            nn.Dropout(0.3),  # Dropout to prevent overfitting\n",
    "\n",
    "            # Hidden layer\n",
    "            nn.Linear(16, 32),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "            nn.Dropout(0.3),\n",
    "\n",
    "            # Output layer - single neuron for binary classification (real or fake)\n",
    "            nn.Linear(32, 1),\n",
    "            nn.Sigmoid()  # Sigmoid to output a probability\n",
    "        )\n",
    "\n",
    "\n",
    "    def forward(self, input):\n",
    "        output = self.main(input)\n",
    "        return output.view(-1, 1).squeeze(1) # Outputs 1D tensor of probabilities \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Custom GAN Dataset\n",
    "from torch.utils.data import Dataset\n",
    "\n",
    "class GANDataset(Dataset):\n",
    "    def __init__(self, dataframe):\n",
    "        self.data_frame = dataframe\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data_frame)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        if torch.is_tensor(idx):\n",
    "            idx = idx.tolist()\n",
    "        \n",
    "        sample = self.data_frame.iloc[idx]\n",
    "        sample = torch.tensor(sample.values, dtype=torch.float32)\n",
    "        return sample\n",
    "\n",
    "GAN_dataset = GANDataset(GAN_train_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# GAN Dataloader \n",
    "from torch.utils.data import DataLoader\n",
    "dataloader = DataLoader(GAN_dataset, batch_size=32, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model Creation\n",
    "generator = Generator().to(device)\n",
    "discriminator = Discriminator().to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I would like to make all of this a reusable pytorch class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0/50][0/1] Loss_D: 1.3781 Loss_G: 0.7046 D(x): 0.4933 D(G(z)): 0.4887 / 0.4946\n",
      "[1/50][0/1] Loss_D: 1.3746 Loss_G: 0.6984 D(x): 0.5032 D(G(z)): 0.4970 / 0.4977\n",
      "[2/50][0/1] Loss_D: 1.3921 Loss_G: 0.7016 D(x): 0.4955 D(G(z)): 0.4980 / 0.4959\n",
      "[3/50][0/1] Loss_D: 1.3934 Loss_G: 0.6984 D(x): 0.4970 D(G(z)): 0.5004 / 0.4975\n",
      "[4/50][0/1] Loss_D: 1.4090 Loss_G: 0.7017 D(x): 0.4913 D(G(z)): 0.5021 / 0.4959\n",
      "[5/50][0/1] Loss_D: 1.3963 Loss_G: 0.7021 D(x): 0.4969 D(G(z)): 0.5016 / 0.4957\n",
      "[6/50][0/1] Loss_D: 1.4033 Loss_G: 0.6998 D(x): 0.4916 D(G(z)): 0.4997 / 0.4969\n",
      "[7/50][0/1] Loss_D: 1.3949 Loss_G: 0.7023 D(x): 0.4927 D(G(z)): 0.4966 / 0.4955\n",
      "[8/50][0/1] Loss_D: 1.3842 Loss_G: 0.6999 D(x): 0.5012 D(G(z)): 0.4999 / 0.4969\n",
      "[9/50][0/1] Loss_D: 1.3884 Loss_G: 0.6965 D(x): 0.4945 D(G(z)): 0.4952 / 0.4985\n",
      "[10/50][0/1] Loss_D: 1.3735 Loss_G: 0.6996 D(x): 0.4973 D(G(z)): 0.4906 / 0.4970\n",
      "[11/50][0/1] Loss_D: 1.3805 Loss_G: 0.6884 D(x): 0.4935 D(G(z)): 0.4902 / 0.5025\n",
      "[12/50][0/1] Loss_D: 1.3740 Loss_G: 0.7038 D(x): 0.4968 D(G(z)): 0.4903 / 0.4948\n",
      "[13/50][0/1] Loss_D: 1.3784 Loss_G: 0.7002 D(x): 0.5003 D(G(z)): 0.4959 / 0.4967\n",
      "[14/50][0/1] Loss_D: 1.3797 Loss_G: 0.7154 D(x): 0.5016 D(G(z)): 0.4980 / 0.4893\n",
      "[15/50][0/1] Loss_D: 1.3789 Loss_G: 0.7118 D(x): 0.4952 D(G(z)): 0.4910 / 0.4910\n",
      "[16/50][0/1] Loss_D: 1.3807 Loss_G: 0.7017 D(x): 0.4997 D(G(z)): 0.4965 / 0.4960\n",
      "[17/50][0/1] Loss_D: 1.3821 Loss_G: 0.7004 D(x): 0.4985 D(G(z)): 0.4962 / 0.4966\n",
      "[18/50][0/1] Loss_D: 1.3912 Loss_G: 0.6962 D(x): 0.4959 D(G(z)): 0.4979 / 0.4985\n",
      "[19/50][0/1] Loss_D: 1.3871 Loss_G: 0.6986 D(x): 0.4989 D(G(z)): 0.4989 / 0.4975\n",
      "[20/50][0/1] Loss_D: 1.3961 Loss_G: 0.7003 D(x): 0.4911 D(G(z)): 0.4955 / 0.4966\n",
      "[21/50][0/1] Loss_D: 1.4056 Loss_G: 0.6898 D(x): 0.4934 D(G(z)): 0.5027 / 0.5018\n",
      "[22/50][0/1] Loss_D: 1.3659 Loss_G: 0.6892 D(x): 0.5019 D(G(z)): 0.4912 / 0.5022\n",
      "[23/50][0/1] Loss_D: 1.3812 Loss_G: 0.7143 D(x): 0.4985 D(G(z)): 0.4955 / 0.4897\n",
      "[24/50][0/1] Loss_D: 1.3947 Loss_G: 0.6960 D(x): 0.4952 D(G(z)): 0.4991 / 0.4988\n",
      "[25/50][0/1] Loss_D: 1.3784 Loss_G: 0.6899 D(x): 0.5021 D(G(z)): 0.4978 / 0.5020\n",
      "[26/50][0/1] Loss_D: 1.3810 Loss_G: 0.6975 D(x): 0.5034 D(G(z)): 0.5004 / 0.4980\n",
      "[27/50][0/1] Loss_D: 1.4008 Loss_G: 0.7057 D(x): 0.4956 D(G(z)): 0.5021 / 0.4940\n",
      "[28/50][0/1] Loss_D: 1.3898 Loss_G: 0.6837 D(x): 0.4947 D(G(z)): 0.4961 / 0.5050\n",
      "[29/50][0/1] Loss_D: 1.3881 Loss_G: 0.7098 D(x): 0.4989 D(G(z)): 0.4995 / 0.4919\n",
      "[30/50][0/1] Loss_D: 1.3776 Loss_G: 0.6942 D(x): 0.4961 D(G(z)): 0.4912 / 0.4997\n",
      "[31/50][0/1] Loss_D: 1.3834 Loss_G: 0.6963 D(x): 0.5003 D(G(z)): 0.4985 / 0.4986\n",
      "[32/50][0/1] Loss_D: 1.3861 Loss_G: 0.7094 D(x): 0.4950 D(G(z)): 0.4946 / 0.4924\n",
      "[33/50][0/1] Loss_D: 1.3771 Loss_G: 0.6978 D(x): 0.4988 D(G(z)): 0.4938 / 0.4980\n",
      "[34/50][0/1] Loss_D: 1.4040 Loss_G: 0.7022 D(x): 0.4986 D(G(z)): 0.5069 / 0.4958\n",
      "[35/50][0/1] Loss_D: 1.3634 Loss_G: 0.6949 D(x): 0.5045 D(G(z)): 0.4925 / 0.4994\n",
      "[36/50][0/1] Loss_D: 1.3811 Loss_G: 0.7000 D(x): 0.4951 D(G(z)): 0.4921 / 0.4967\n",
      "[37/50][0/1] Loss_D: 1.3654 Loss_G: 0.6906 D(x): 0.5028 D(G(z)): 0.4919 / 0.5014\n",
      "[38/50][0/1] Loss_D: 1.3836 Loss_G: 0.7039 D(x): 0.4976 D(G(z)): 0.4959 / 0.4950\n",
      "[39/50][0/1] Loss_D: 1.3796 Loss_G: 0.6958 D(x): 0.5076 D(G(z)): 0.5038 / 0.4988\n",
      "[40/50][0/1] Loss_D: 1.3875 Loss_G: 0.6798 D(x): 0.4963 D(G(z)): 0.4965 / 0.5070\n",
      "[41/50][0/1] Loss_D: 1.3845 Loss_G: 0.7027 D(x): 0.5015 D(G(z)): 0.5004 / 0.4954\n",
      "[42/50][0/1] Loss_D: 1.3972 Loss_G: 0.6866 D(x): 0.4947 D(G(z)): 0.4999 / 0.5035\n",
      "[43/50][0/1] Loss_D: 1.3897 Loss_G: 0.6986 D(x): 0.4973 D(G(z)): 0.4988 / 0.4975\n",
      "[44/50][0/1] Loss_D: 1.3791 Loss_G: 0.6987 D(x): 0.5009 D(G(z)): 0.4972 / 0.4975\n",
      "[45/50][0/1] Loss_D: 1.4038 Loss_G: 0.7030 D(x): 0.4972 D(G(z)): 0.5055 / 0.4953\n",
      "[46/50][0/1] Loss_D: 1.3886 Loss_G: 0.7081 D(x): 0.4954 D(G(z)): 0.4961 / 0.4927\n",
      "[47/50][0/1] Loss_D: 1.3805 Loss_G: 0.6985 D(x): 0.5005 D(G(z)): 0.4972 / 0.4974\n",
      "[48/50][0/1] Loss_D: 1.3875 Loss_G: 0.6903 D(x): 0.4972 D(G(z)): 0.4974 / 0.5017\n",
      "[49/50][0/1] Loss_D: 1.3772 Loss_G: 0.7092 D(x): 0.5032 D(G(z)): 0.4982 / 0.4923\n"
     ]
    }
   ],
   "source": [
    "# Setup Adam optimizers for both G and D\n",
    "import torch.optim as optim\n",
    "optimizerD = optim.Adam(discriminator.parameters(), lr=0.0002, betas=(0.5, 0.999))\n",
    "optimizerG = optim.Adam(generator.parameters(), lr=0.0002, betas=(0.5, 0.999))\n",
    "\n",
    "# Initialize the ``BCELoss`` function\n",
    "criterion = nn.BCELoss()\n",
    "\n",
    "generator_losses = []\n",
    "discriminator_losses = []\n",
    "epochs = 50\n",
    "\n",
    "def train_GAN():\n",
    "    for epoch in range(epochs):\n",
    "        for i, data in enumerate(dataloader, 0):\n",
    "            \n",
    "            # Seperate data and convert to current device\n",
    "            chemical_data = data[:, :9]\n",
    "            sensory_data = data[:, 9:]\n",
    "\n",
    "            chemical_data = chemical_data.to(device)\n",
    "            sensory_data = sensory_data.to(device)\n",
    "\n",
    "            # Find current batch size\n",
    "            current_batch_size = chemical_data.size(0)\n",
    "\n",
    "            # Create labels for real and generated data\n",
    "            real_label = torch.full((current_batch_size,), 1., dtype=torch.float, device=device) # Creates a tensor of labels indiciated data is real\n",
    "            fake_label = torch.full((current_batch_size,), 0. , dtype=torch.float, device=device) # Creates a tensor of labels indidicated data is generated\n",
    "\n",
    "            # Update Discriminator - Clear accumulated gradients from previous pass\n",
    "            discriminator.zero_grad()\n",
    "\n",
    "            # Train discriminator with real data w/ some notes for myself\n",
    "            real_data = torch.cat((chemical_data, sensory_data), 1) # Same as pd.concat(axis=1)\n",
    "            output = discriminator(real_data).view(-1) # Same as numpy.reshape(-1,1), used to calculate loss\n",
    "            errD_real = criterion(output, real_label)\n",
    "            errD_real.backward() # Perform backpropogation based on errD_real\n",
    "            D_x = output.mean().item() # Outputs average discriminator prediction\n",
    "\n",
    "            # Train discriminator with fake data\n",
    "            fake_sensory_data = generator(chemical_data) \n",
    "            fake_data = torch.cat((chemical_data, fake_sensory_data), 1) # Create a dataset of real chemical data but generated sensory data\n",
    "            output = discriminator(fake_data.detach()).view(-1) # detach is used to ensure gradients aren't computed for the generator during the discriminators backwards pass\n",
    "            errD_fake = criterion(output, fake_label)\n",
    "            errD_fake.backward()\n",
    "            D_G_z1 = output.mean().item() # Outputs average discriminator prediction for fake data\n",
    "\n",
    "            # Update Discriminator weights\n",
    "            errD = errD_real + errD_fake\n",
    "            optimizerD.step()\n",
    "\n",
    "            # Update Generator\n",
    "            generator.zero_grad()\n",
    "            output = discriminator(fake_data).view(-1)\n",
    "            errG = criterion(output, real_label)\n",
    "            errG.backward()\n",
    "            D_G_z2 = output.mean().item()\n",
    "\n",
    "            # Update Generate weights\n",
    "            optimizerG.step()\n",
    "\n",
    "            # Print training log\n",
    "            if i % 50 == 0: # Prints every 50 batches, might change\n",
    "                print(f'[{epoch}/{epochs}][{i}/{len(dataloader)}] '\n",
    "                    f'Loss_D: {errD.item():.4f} Loss_G: {errG.item():.4f} '\n",
    "                    f'D(x): {D_x:.4f} D(G(z)): {D_G_z1:.4f} / {D_G_z2:.4f}')\n",
    "                \n",
    "            # Save losses\n",
    "            generator_losses.append(errG.item())\n",
    "            discriminator_losses.append(errD.item())\n",
    "\n",
    "train_GAN()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1200x700 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def plot_GAN_loss():\n",
    "    plt.figure(figsize=(12,7))\n",
    "    plt.title('Generator and Discriminator Loss During Training')\n",
    "    plt.plot(generator_losses, label='G')\n",
    "    plt.plot(discriminator_losses, label='D')\n",
    "    plt.legend()\n",
    "    plt.xlabel('Iterations')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.show()\n",
    "\n",
    "plot_GAN_loss()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Define rows that need to be imputed\n",
    "These are the rows currently missing sensory data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data_to_impute = torch.tensor(scaled_train_df[(scaled_train_df[sensory_evaluations].isna()).all(axis=1)][chemicals].values)\n",
    "test_data_to_impute = torch.tensor(scaled_test_df[(scaled_test_df[sensory_evaluations].isna()).all(axis=1)][chemicals].values)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Impute missing sensory data with the GAN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "generator.eval()\n",
    "\n",
    "with torch.no_grad():\n",
    "    imputed_sensory_data_train = pd.DataFrame(generator(train_data_to_impute.float()).numpy(), columns=sensory_evaluations)\n",
    "    imputed_sensory_data_test = pd.DataFrame(generator(test_data_to_impute.float()).numpy(), columns=sensory_evaluations)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Taste</th>\n",
       "      <th>Appearance</th>\n",
       "      <th>Aroma</th>\n",
       "      <th>Liqour color</th>\n",
       "      <th>Infused leaf</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.626769</td>\n",
       "      <td>0.414748</td>\n",
       "      <td>0.573203</td>\n",
       "      <td>0.667516</td>\n",
       "      <td>0.591182</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.761297</td>\n",
       "      <td>0.391872</td>\n",
       "      <td>0.825790</td>\n",
       "      <td>0.453418</td>\n",
       "      <td>0.880086</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.760653</td>\n",
       "      <td>0.392795</td>\n",
       "      <td>0.823049</td>\n",
       "      <td>0.458809</td>\n",
       "      <td>0.876479</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.665207</td>\n",
       "      <td>0.419440</td>\n",
       "      <td>0.576511</td>\n",
       "      <td>0.627948</td>\n",
       "      <td>0.584144</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.684942</td>\n",
       "      <td>0.429010</td>\n",
       "      <td>0.741914</td>\n",
       "      <td>0.488619</td>\n",
       "      <td>0.802794</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0.708346</td>\n",
       "      <td>0.386132</td>\n",
       "      <td>0.601727</td>\n",
       "      <td>0.621220</td>\n",
       "      <td>0.596056</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0.755847</td>\n",
       "      <td>0.408579</td>\n",
       "      <td>0.830897</td>\n",
       "      <td>0.479655</td>\n",
       "      <td>0.831309</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0.750563</td>\n",
       "      <td>0.395468</td>\n",
       "      <td>0.813053</td>\n",
       "      <td>0.458434</td>\n",
       "      <td>0.875597</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>0.539499</td>\n",
       "      <td>0.439985</td>\n",
       "      <td>0.566619</td>\n",
       "      <td>0.630822</td>\n",
       "      <td>0.708074</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>0.592084</td>\n",
       "      <td>0.449198</td>\n",
       "      <td>0.606819</td>\n",
       "      <td>0.557042</td>\n",
       "      <td>0.642096</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>0.756116</td>\n",
       "      <td>0.392878</td>\n",
       "      <td>0.820089</td>\n",
       "      <td>0.458499</td>\n",
       "      <td>0.877459</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>0.682296</td>\n",
       "      <td>0.413683</td>\n",
       "      <td>0.475331</td>\n",
       "      <td>0.483786</td>\n",
       "      <td>0.219667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>0.751922</td>\n",
       "      <td>0.387013</td>\n",
       "      <td>0.793193</td>\n",
       "      <td>0.500856</td>\n",
       "      <td>0.803192</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>0.681925</td>\n",
       "      <td>0.414998</td>\n",
       "      <td>0.719214</td>\n",
       "      <td>0.584552</td>\n",
       "      <td>0.689661</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>0.744393</td>\n",
       "      <td>0.416353</td>\n",
       "      <td>0.804538</td>\n",
       "      <td>0.500950</td>\n",
       "      <td>0.824494</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>0.759037</td>\n",
       "      <td>0.404913</td>\n",
       "      <td>0.823287</td>\n",
       "      <td>0.482073</td>\n",
       "      <td>0.829829</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>0.542476</td>\n",
       "      <td>0.428729</td>\n",
       "      <td>0.578494</td>\n",
       "      <td>0.631332</td>\n",
       "      <td>0.739503</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>0.760510</td>\n",
       "      <td>0.392309</td>\n",
       "      <td>0.823197</td>\n",
       "      <td>0.455706</td>\n",
       "      <td>0.877996</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>0.733098</td>\n",
       "      <td>0.426485</td>\n",
       "      <td>0.787408</td>\n",
       "      <td>0.521502</td>\n",
       "      <td>0.778145</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>0.607696</td>\n",
       "      <td>0.432245</td>\n",
       "      <td>0.606896</td>\n",
       "      <td>0.548870</td>\n",
       "      <td>0.661878</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>0.760217</td>\n",
       "      <td>0.423241</td>\n",
       "      <td>0.832703</td>\n",
       "      <td>0.483965</td>\n",
       "      <td>0.843028</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>0.508915</td>\n",
       "      <td>0.492752</td>\n",
       "      <td>0.480926</td>\n",
       "      <td>0.581576</td>\n",
       "      <td>0.544139</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>0.686252</td>\n",
       "      <td>0.338991</td>\n",
       "      <td>0.574345</td>\n",
       "      <td>0.582435</td>\n",
       "      <td>0.602201</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>0.747889</td>\n",
       "      <td>0.409800</td>\n",
       "      <td>0.811666</td>\n",
       "      <td>0.499178</td>\n",
       "      <td>0.810657</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>0.748480</td>\n",
       "      <td>0.396449</td>\n",
       "      <td>0.810764</td>\n",
       "      <td>0.460694</td>\n",
       "      <td>0.874165</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>0.751102</td>\n",
       "      <td>0.395651</td>\n",
       "      <td>0.812379</td>\n",
       "      <td>0.460162</td>\n",
       "      <td>0.874064</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>0.677384</td>\n",
       "      <td>0.497835</td>\n",
       "      <td>0.697370</td>\n",
       "      <td>0.593179</td>\n",
       "      <td>0.668586</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>0.558365</td>\n",
       "      <td>0.439125</td>\n",
       "      <td>0.556233</td>\n",
       "      <td>0.598423</td>\n",
       "      <td>0.690028</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>0.752715</td>\n",
       "      <td>0.406480</td>\n",
       "      <td>0.825420</td>\n",
       "      <td>0.457636</td>\n",
       "      <td>0.870351</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>0.757667</td>\n",
       "      <td>0.409839</td>\n",
       "      <td>0.801439</td>\n",
       "      <td>0.489601</td>\n",
       "      <td>0.825165</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>0.568975</td>\n",
       "      <td>0.443579</td>\n",
       "      <td>0.515037</td>\n",
       "      <td>0.634798</td>\n",
       "      <td>0.625487</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>0.748052</td>\n",
       "      <td>0.397009</td>\n",
       "      <td>0.808777</td>\n",
       "      <td>0.456995</td>\n",
       "      <td>0.873591</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>0.523510</td>\n",
       "      <td>0.437381</td>\n",
       "      <td>0.515472</td>\n",
       "      <td>0.645315</td>\n",
       "      <td>0.730845</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>0.750549</td>\n",
       "      <td>0.394737</td>\n",
       "      <td>0.815062</td>\n",
       "      <td>0.458499</td>\n",
       "      <td>0.876846</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>0.746921</td>\n",
       "      <td>0.429950</td>\n",
       "      <td>0.785761</td>\n",
       "      <td>0.502177</td>\n",
       "      <td>0.738765</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>0.654631</td>\n",
       "      <td>0.411820</td>\n",
       "      <td>0.665372</td>\n",
       "      <td>0.563056</td>\n",
       "      <td>0.673260</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36</th>\n",
       "      <td>0.746206</td>\n",
       "      <td>0.405981</td>\n",
       "      <td>0.797559</td>\n",
       "      <td>0.500869</td>\n",
       "      <td>0.827827</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37</th>\n",
       "      <td>0.591265</td>\n",
       "      <td>0.424886</td>\n",
       "      <td>0.592508</td>\n",
       "      <td>0.619391</td>\n",
       "      <td>0.683224</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38</th>\n",
       "      <td>0.709953</td>\n",
       "      <td>0.439626</td>\n",
       "      <td>0.747316</td>\n",
       "      <td>0.521181</td>\n",
       "      <td>0.703051</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39</th>\n",
       "      <td>0.710969</td>\n",
       "      <td>0.403345</td>\n",
       "      <td>0.746504</td>\n",
       "      <td>0.541840</td>\n",
       "      <td>0.712358</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40</th>\n",
       "      <td>0.675481</td>\n",
       "      <td>0.419369</td>\n",
       "      <td>0.708010</td>\n",
       "      <td>0.581103</td>\n",
       "      <td>0.668115</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41</th>\n",
       "      <td>0.774754</td>\n",
       "      <td>0.387380</td>\n",
       "      <td>0.850324</td>\n",
       "      <td>0.442259</td>\n",
       "      <td>0.878123</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42</th>\n",
       "      <td>0.728948</td>\n",
       "      <td>0.417652</td>\n",
       "      <td>0.785395</td>\n",
       "      <td>0.458346</td>\n",
       "      <td>0.848956</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43</th>\n",
       "      <td>0.605327</td>\n",
       "      <td>0.408698</td>\n",
       "      <td>0.553995</td>\n",
       "      <td>0.637586</td>\n",
       "      <td>0.643843</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44</th>\n",
       "      <td>0.755359</td>\n",
       "      <td>0.394930</td>\n",
       "      <td>0.816495</td>\n",
       "      <td>0.458254</td>\n",
       "      <td>0.875273</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>45</th>\n",
       "      <td>0.681360</td>\n",
       "      <td>0.376889</td>\n",
       "      <td>0.552476</td>\n",
       "      <td>0.656729</td>\n",
       "      <td>0.623413</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>46</th>\n",
       "      <td>0.766849</td>\n",
       "      <td>0.375663</td>\n",
       "      <td>0.840736</td>\n",
       "      <td>0.463206</td>\n",
       "      <td>0.856565</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>47</th>\n",
       "      <td>0.759326</td>\n",
       "      <td>0.406703</td>\n",
       "      <td>0.817985</td>\n",
       "      <td>0.476597</td>\n",
       "      <td>0.848516</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>48</th>\n",
       "      <td>0.752147</td>\n",
       "      <td>0.408978</td>\n",
       "      <td>0.822303</td>\n",
       "      <td>0.487250</td>\n",
       "      <td>0.826856</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49</th>\n",
       "      <td>0.758519</td>\n",
       "      <td>0.392785</td>\n",
       "      <td>0.822701</td>\n",
       "      <td>0.457986</td>\n",
       "      <td>0.878001</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       Taste  Appearance     Aroma  Liqour color  Infused leaf\n",
       "0   0.626769    0.414748  0.573203      0.667516      0.591182\n",
       "1   0.761297    0.391872  0.825790      0.453418      0.880086\n",
       "2   0.760653    0.392795  0.823049      0.458809      0.876479\n",
       "3   0.665207    0.419440  0.576511      0.627948      0.584144\n",
       "4   0.684942    0.429010  0.741914      0.488619      0.802794\n",
       "5   0.708346    0.386132  0.601727      0.621220      0.596056\n",
       "6   0.755847    0.408579  0.830897      0.479655      0.831309\n",
       "7   0.750563    0.395468  0.813053      0.458434      0.875597\n",
       "8   0.539499    0.439985  0.566619      0.630822      0.708074\n",
       "9   0.592084    0.449198  0.606819      0.557042      0.642096\n",
       "10  0.756116    0.392878  0.820089      0.458499      0.877459\n",
       "11  0.682296    0.413683  0.475331      0.483786      0.219667\n",
       "12  0.751922    0.387013  0.793193      0.500856      0.803192\n",
       "13  0.681925    0.414998  0.719214      0.584552      0.689661\n",
       "14  0.744393    0.416353  0.804538      0.500950      0.824494\n",
       "15  0.759037    0.404913  0.823287      0.482073      0.829829\n",
       "16  0.542476    0.428729  0.578494      0.631332      0.739503\n",
       "17  0.760510    0.392309  0.823197      0.455706      0.877996\n",
       "18  0.733098    0.426485  0.787408      0.521502      0.778145\n",
       "19  0.607696    0.432245  0.606896      0.548870      0.661878\n",
       "20  0.760217    0.423241  0.832703      0.483965      0.843028\n",
       "21  0.508915    0.492752  0.480926      0.581576      0.544139\n",
       "22  0.686252    0.338991  0.574345      0.582435      0.602201\n",
       "23  0.747889    0.409800  0.811666      0.499178      0.810657\n",
       "24  0.748480    0.396449  0.810764      0.460694      0.874165\n",
       "25  0.751102    0.395651  0.812379      0.460162      0.874064\n",
       "26  0.677384    0.497835  0.697370      0.593179      0.668586\n",
       "27  0.558365    0.439125  0.556233      0.598423      0.690028\n",
       "28  0.752715    0.406480  0.825420      0.457636      0.870351\n",
       "29  0.757667    0.409839  0.801439      0.489601      0.825165\n",
       "30  0.568975    0.443579  0.515037      0.634798      0.625487\n",
       "31  0.748052    0.397009  0.808777      0.456995      0.873591\n",
       "32  0.523510    0.437381  0.515472      0.645315      0.730845\n",
       "33  0.750549    0.394737  0.815062      0.458499      0.876846\n",
       "34  0.746921    0.429950  0.785761      0.502177      0.738765\n",
       "35  0.654631    0.411820  0.665372      0.563056      0.673260\n",
       "36  0.746206    0.405981  0.797559      0.500869      0.827827\n",
       "37  0.591265    0.424886  0.592508      0.619391      0.683224\n",
       "38  0.709953    0.439626  0.747316      0.521181      0.703051\n",
       "39  0.710969    0.403345  0.746504      0.541840      0.712358\n",
       "40  0.675481    0.419369  0.708010      0.581103      0.668115\n",
       "41  0.774754    0.387380  0.850324      0.442259      0.878123\n",
       "42  0.728948    0.417652  0.785395      0.458346      0.848956\n",
       "43  0.605327    0.408698  0.553995      0.637586      0.643843\n",
       "44  0.755359    0.394930  0.816495      0.458254      0.875273\n",
       "45  0.681360    0.376889  0.552476      0.656729      0.623413\n",
       "46  0.766849    0.375663  0.840736      0.463206      0.856565\n",
       "47  0.759326    0.406703  0.817985      0.476597      0.848516\n",
       "48  0.752147    0.408978  0.822303      0.487250      0.826856\n",
       "49  0.758519    0.392785  0.822701      0.457986      0.878001"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "imputed_sensory_data_train # Note, index is not right not simply concat on axis=1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Concatenate imputed values to host dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = pd.concat((scaled_train_df[(scaled_train_df[sensory_evaluations].isna()).all(axis=1)][chemicals], imputed_sensory_data_train), axis=1)\n",
    "test_df = pd.concat((scaled_test_df[(scaled_test_df[sensory_evaluations].isna()).all(axis=1)][chemicals], imputed_sensory_data_test), axis=1)\n",
    "# This is not correct right now"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature Engineering\n",
    "\n",
    "A derived feature called overall score will be produced from all sensory scores so the models only have to predict a single response."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Derived Feature: Overall Sensory Score\n",
    "def compute_sensory_score(df):\n",
    "    df['Overall Sensory Score'] = df['Taste'] + df['Appearance'] + df['Aroma'] + df['Liqour color'] + df['Infused leaf']\n",
    "    df = df.drop(columns=['Taste','Appearance','Aroma','Liqour color','Infused leaf'])\n",
    "    return df\n",
    "\n",
    "train_df = compute_sensory_score(train_df)\n",
    "test_df = compute_sensory_score(test_df)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PCA (Principal Component Analysis)\n",
    "\n",
    "Because the high-dimensional nature of the data, PCA will be used to reduce the number of features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Input X contains NaN.\nKernelPCA does not accept missing values encoded as NaN natively. For supervised learning, you might want to consider sklearn.ensemble.HistGradientBoostingClassifier and Regressor which accept missing values encoded as NaNs natively. Alternatively, it is possible to preprocess the data, for instance by using an imputer transformer in a pipeline or drop samples with missing values. See https://scikit-learn.org/stable/modules/impute.html You can find a list of all estimators that handle NaN values at the following page: https://scikit-learn.org/stable/modules/impute.html#estimators-that-handle-nan-values",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[86], line 5\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01msklearn\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdecomposition\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m KernelPCA\n\u001b[1;32m      3\u001b[0m pca \u001b[38;5;241m=\u001b[39m KernelPCA(n_components\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m2\u001b[39m)\n\u001b[0;32m----> 5\u001b[0m train_X \u001b[38;5;241m=\u001b[39m \u001b[43mpca\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit_transform\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrain_df\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdrop\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcolumns\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mOverall Sensory Score\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      6\u001b[0m test_X \u001b[38;5;241m=\u001b[39m pca\u001b[38;5;241m.\u001b[39mtransform(test_df\u001b[38;5;241m.\u001b[39mdrop(columns\u001b[38;5;241m=\u001b[39m[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mOverall Sensory Score\u001b[39m\u001b[38;5;124m'\u001b[39m]))\n\u001b[1;32m      8\u001b[0m train_y \u001b[38;5;241m=\u001b[39m train_df[[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mOverall Sensory Score\u001b[39m\u001b[38;5;124m'\u001b[39m]]\n",
      "File \u001b[0;32m~/Documents/Github/DTSC-691-Capstone-Project/.venv/lib/python3.11/site-packages/sklearn/utils/_set_output.py:273\u001b[0m, in \u001b[0;36m_wrap_method_output.<locals>.wrapped\u001b[0;34m(self, X, *args, **kwargs)\u001b[0m\n\u001b[1;32m    271\u001b[0m \u001b[38;5;129m@wraps\u001b[39m(f)\n\u001b[1;32m    272\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mwrapped\u001b[39m(\u001b[38;5;28mself\u001b[39m, X, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[0;32m--> 273\u001b[0m     data_to_wrap \u001b[38;5;241m=\u001b[39m \u001b[43mf\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    274\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(data_to_wrap, \u001b[38;5;28mtuple\u001b[39m):\n\u001b[1;32m    275\u001b[0m         \u001b[38;5;66;03m# only wrap the first output for cross decomposition\u001b[39;00m\n\u001b[1;32m    276\u001b[0m         return_tuple \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m    277\u001b[0m             _wrap_data_with_container(method, data_to_wrap[\u001b[38;5;241m0\u001b[39m], X, \u001b[38;5;28mself\u001b[39m),\n\u001b[1;32m    278\u001b[0m             \u001b[38;5;241m*\u001b[39mdata_to_wrap[\u001b[38;5;241m1\u001b[39m:],\n\u001b[1;32m    279\u001b[0m         )\n",
      "File \u001b[0;32m~/Documents/Github/DTSC-691-Capstone-Project/.venv/lib/python3.11/site-packages/sklearn/decomposition/_kernel_pca.py:472\u001b[0m, in \u001b[0;36mKernelPCA.fit_transform\u001b[0;34m(self, X, y, **params)\u001b[0m\n\u001b[1;32m    451\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mfit_transform\u001b[39m(\u001b[38;5;28mself\u001b[39m, X, y\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mparams):\n\u001b[1;32m    452\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Fit the model from data in X and transform X.\u001b[39;00m\n\u001b[1;32m    453\u001b[0m \n\u001b[1;32m    454\u001b[0m \u001b[38;5;124;03m    Parameters\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    470\u001b[0m \u001b[38;5;124;03m        Returns the instance itself.\u001b[39;00m\n\u001b[1;32m    471\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 472\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mparams\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    474\u001b[0m     \u001b[38;5;66;03m# no need to use the kernel to transform X, use shortcut expression\u001b[39;00m\n\u001b[1;32m    475\u001b[0m     X_transformed \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39meigenvectors_ \u001b[38;5;241m*\u001b[39m np\u001b[38;5;241m.\u001b[39msqrt(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39meigenvalues_)\n",
      "File \u001b[0;32m~/Documents/Github/DTSC-691-Capstone-Project/.venv/lib/python3.11/site-packages/sklearn/base.py:1351\u001b[0m, in \u001b[0;36m_fit_context.<locals>.decorator.<locals>.wrapper\u001b[0;34m(estimator, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1344\u001b[0m     estimator\u001b[38;5;241m.\u001b[39m_validate_params()\n\u001b[1;32m   1346\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m config_context(\n\u001b[1;32m   1347\u001b[0m     skip_parameter_validation\u001b[38;5;241m=\u001b[39m(\n\u001b[1;32m   1348\u001b[0m         prefer_skip_nested_validation \u001b[38;5;129;01mor\u001b[39;00m global_skip_validation\n\u001b[1;32m   1349\u001b[0m     )\n\u001b[1;32m   1350\u001b[0m ):\n\u001b[0;32m-> 1351\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfit_method\u001b[49m\u001b[43m(\u001b[49m\u001b[43mestimator\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Documents/Github/DTSC-691-Capstone-Project/.venv/lib/python3.11/site-packages/sklearn/decomposition/_kernel_pca.py:436\u001b[0m, in \u001b[0;36mKernelPCA.fit\u001b[0;34m(self, X, y)\u001b[0m\n\u001b[1;32m    434\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfit_inverse_transform \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mkernel \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mprecomputed\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m    435\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCannot fit_inverse_transform with a precomputed kernel.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m--> 436\u001b[0m X \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_validate_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maccept_sparse\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcsr\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcopy\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcopy_X\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    437\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgamma_ \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m \u001b[38;5;241m/\u001b[39m X\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m1\u001b[39m] \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgamma \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgamma\n\u001b[1;32m    438\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_centerer \u001b[38;5;241m=\u001b[39m KernelCenterer()\u001b[38;5;241m.\u001b[39mset_output(transform\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdefault\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m~/Documents/Github/DTSC-691-Capstone-Project/.venv/lib/python3.11/site-packages/sklearn/base.py:633\u001b[0m, in \u001b[0;36mBaseEstimator._validate_data\u001b[0;34m(self, X, y, reset, validate_separately, cast_to_ndarray, **check_params)\u001b[0m\n\u001b[1;32m    631\u001b[0m         out \u001b[38;5;241m=\u001b[39m X, y\n\u001b[1;32m    632\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m no_val_X \u001b[38;5;129;01mand\u001b[39;00m no_val_y:\n\u001b[0;32m--> 633\u001b[0m     out \u001b[38;5;241m=\u001b[39m \u001b[43mcheck_array\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minput_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mX\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mcheck_params\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    634\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m no_val_X \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m no_val_y:\n\u001b[1;32m    635\u001b[0m     out \u001b[38;5;241m=\u001b[39m _check_y(y, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mcheck_params)\n",
      "File \u001b[0;32m~/Documents/Github/DTSC-691-Capstone-Project/.venv/lib/python3.11/site-packages/sklearn/utils/validation.py:1003\u001b[0m, in \u001b[0;36mcheck_array\u001b[0;34m(array, accept_sparse, accept_large_sparse, dtype, order, copy, force_all_finite, ensure_2d, allow_nd, ensure_min_samples, ensure_min_features, estimator, input_name)\u001b[0m\n\u001b[1;32m    997\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    998\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFound array with dim \u001b[39m\u001b[38;5;132;01m%d\u001b[39;00m\u001b[38;5;124m. \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m expected <= 2.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    999\u001b[0m         \u001b[38;5;241m%\u001b[39m (array\u001b[38;5;241m.\u001b[39mndim, estimator_name)\n\u001b[1;32m   1000\u001b[0m     )\n\u001b[1;32m   1002\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m force_all_finite:\n\u001b[0;32m-> 1003\u001b[0m     \u001b[43m_assert_all_finite\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1004\u001b[0m \u001b[43m        \u001b[49m\u001b[43marray\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1005\u001b[0m \u001b[43m        \u001b[49m\u001b[43minput_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minput_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1006\u001b[0m \u001b[43m        \u001b[49m\u001b[43mestimator_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mestimator_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1007\u001b[0m \u001b[43m        \u001b[49m\u001b[43mallow_nan\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mforce_all_finite\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m==\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mallow-nan\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1008\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1010\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m copy:\n\u001b[1;32m   1011\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m _is_numpy_namespace(xp):\n\u001b[1;32m   1012\u001b[0m         \u001b[38;5;66;03m# only make a copy if `array` and `array_orig` may share memory`\u001b[39;00m\n",
      "File \u001b[0;32m~/Documents/Github/DTSC-691-Capstone-Project/.venv/lib/python3.11/site-packages/sklearn/utils/validation.py:126\u001b[0m, in \u001b[0;36m_assert_all_finite\u001b[0;34m(X, allow_nan, msg_dtype, estimator_name, input_name)\u001b[0m\n\u001b[1;32m    123\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m first_pass_isfinite:\n\u001b[1;32m    124\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m\n\u001b[0;32m--> 126\u001b[0m \u001b[43m_assert_all_finite_element_wise\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    127\u001b[0m \u001b[43m    \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    128\u001b[0m \u001b[43m    \u001b[49m\u001b[43mxp\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mxp\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    129\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_nan\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mallow_nan\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    130\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmsg_dtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmsg_dtype\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    131\u001b[0m \u001b[43m    \u001b[49m\u001b[43mestimator_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mestimator_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    132\u001b[0m \u001b[43m    \u001b[49m\u001b[43minput_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minput_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    133\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Documents/Github/DTSC-691-Capstone-Project/.venv/lib/python3.11/site-packages/sklearn/utils/validation.py:175\u001b[0m, in \u001b[0;36m_assert_all_finite_element_wise\u001b[0;34m(X, xp, allow_nan, msg_dtype, estimator_name, input_name)\u001b[0m\n\u001b[1;32m    158\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m estimator_name \u001b[38;5;129;01mand\u001b[39;00m input_name \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mX\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m has_nan_error:\n\u001b[1;32m    159\u001b[0m     \u001b[38;5;66;03m# Improve the error message on how to handle missing values in\u001b[39;00m\n\u001b[1;32m    160\u001b[0m     \u001b[38;5;66;03m# scikit-learn.\u001b[39;00m\n\u001b[1;32m    161\u001b[0m     msg_err \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m    162\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00mestimator_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m does not accept missing values\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    163\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m encoded as NaN natively. For supervised learning, you might want\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    173\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m#estimators-that-handle-nan-values\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    174\u001b[0m     )\n\u001b[0;32m--> 175\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(msg_err)\n",
      "\u001b[0;31mValueError\u001b[0m: Input X contains NaN.\nKernelPCA does not accept missing values encoded as NaN natively. For supervised learning, you might want to consider sklearn.ensemble.HistGradientBoostingClassifier and Regressor which accept missing values encoded as NaNs natively. Alternatively, it is possible to preprocess the data, for instance by using an imputer transformer in a pipeline or drop samples with missing values. See https://scikit-learn.org/stable/modules/impute.html You can find a list of all estimators that handle NaN values at the following page: https://scikit-learn.org/stable/modules/impute.html#estimators-that-handle-nan-values"
     ]
    }
   ],
   "source": [
    "from sklearn.decomposition import KernelPCA\n",
    "\n",
    "pca = KernelPCA(n_components=2)\n",
    "\n",
    "train_X = pca.fit_transform(train_df.drop(columns=['Overall Sensory Score']))\n",
    "test_X = pca.transform(test_df.drop(columns=['Overall Sensory Score']))\n",
    "\n",
    "train_y = train_df[['Overall Sensory Score']]\n",
    "test_y = test_df[['Overall Sensory Socre']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_explained_variance():  \n",
    "    plt.figure(figsize=(12, 7))\n",
    "    plt.xlabel('Principal Components')\n",
    "    plt.ylabel('Explained Variance')\n",
    "    plt.title('Explained Variance by Each Principal Components')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DBSCAN\n",
    "\n",
    "Pytorch does not have a built in DBSCAN class, instead we will use scikit learn's implementation. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model Creation\n",
    "from sklearn.cluster import DBSCAN\n",
    "GridSearchCV_DBSCAN = DBSCAN()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyperparameter Selection\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "parameters_dbscan = {\n",
    "    'eps': [_ for _ in np.arange(.5, 5.5, .5)],\n",
    "    'min_samples': [_ for _ in np.arange(2, 6, 1)],\n",
    "    'metric': ['euclidian' , 'sqeuclidian', 'cosine', 'cityblock', 'manhattan'],\n",
    "    'algorithm': ['auto', 'ball_tree', 'kd_tree', 'brute'], \n",
    "    'leaf_size': [_ for _ in np.arange(20, 41, 1)],\n",
    "    'n_jobs': [-1]\n",
    "}\n",
    "\n",
    "DBSCAN_GridSearchCV = GridSearchCV(GridSearchCV_DBSCAN, parameters_dbscan)\n",
    "DBSCAN_GridSearchCV.fit(train_X, train_y)\n",
    "DBSCAN_GridSearchCV.best_params_, DBSCAN_GridSearchCV.best_score_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model Training\n",
    "Model_DBSCAN = DBSCAN() # Insert model parameters from above here."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ensemble Method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Multilayer Perceptron\n",
    "This multilayer perceptron outputs a overall sensory score based on input chemicals."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "notes: I ultimately want someone to put in a tea and see if it is generally likeable. This returns the scaled score so I need to return that scaled score into an interpretable one. Next I need to see how I would integrate this model into an interactie web app. Think about that..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model Creation\n",
    "class MLP(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()  \n",
    "        self.mean = nn.Sequential(\n",
    "            # First Layer\n",
    "            nn.Linear(9, 16),\n",
    "            nn.ReLU(),\n",
    "\n",
    "            # Second Layer\n",
    "            nn.Linear(16, 32),\n",
    "            nn.ReLU(),\n",
    "\n",
    "            # Output Layer\n",
    "            nn.Linear(32, 1)\n",
    "            nn.ReLU() # Used because the output is greater than 1\n",
    "        )\n",
    "\n",
    "    def forward(self, input):\n",
    "        output = self.main(input)\n",
    "        return output\n",
    "\n",
    "GridSearchCV_MLP = MLP()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training Loop Creation - very rough creation\n",
    "def train_MLP(model, X, y):\n",
    "    loss_fn = nn.MSELoss()\n",
    "    optimizer = optim.Adam(model.parameters(), lr=0.0002, betas=(0.5, 0.999))\n",
    "    for epoch in range(epochs):\n",
    "        y_pred = model(X)\n",
    "        loss = loss_fn(y_pred, y)\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyperparameter Selection\n",
    "parameters_mlp = {\n",
    "    '':[]\n",
    "}\n",
    "\n",
    "# https://machinelearningmastery.com/how-to-grid-search-hyperparameters-for-pytorch-models/\n",
    "\n",
    "MLP_GridSearchCV = GridSearchCV(GridSearchCV_MLP, parameters_mlp)\n",
    "MLP_GridSearchCV.fit(train_X, train_y)\n",
    "MLP_GridSearchCV.best_params_, MLP_GridSearchCV.best_score_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model Training\n",
    "Model_MLP = MLP() # Insert model parameters from above here."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Convolutiontional Neural Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model Creation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyperparameter Selection\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model Training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Visualization and Comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_models():\n",
    "    pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Export model\n",
    "\n",
    "This model will be deployed in Python via a REST API with Flask. The Flask web app will then be deployed to a Linux web server using Unicorn."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_models():\n",
    "    torch.save(Model_MLP, 'mlp.pickle')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
